{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P30fzHNnD3QT"
   },
   "source": [
    "# üö®üö®üö® LEIA-ME üö®üö®üö®\n",
    "\n",
    "Se estiver utilizando o Google Colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCZ5V8nnbS4w"
   },
   "source": [
    "Antes de iniciar a execu√ß√£o do c√≥digo, √© importante alterar a vers√£o do runtime para a que possui o Python na vers√£o 3.11.*, que atualmente segue sendo a vers√£o \"2025.07\".\n",
    "\n",
    "Logo abaixo mostro como alterar a vers√£o do Runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOES4keEbhWl"
   },
   "source": [
    "![Acesse o Runtime](https://i.imgur.com/EOQ8KvA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbDDzVIJch7m"
   },
   "source": [
    "Clique no menu Runtime (Ambiente de execu√ß√£o). Vai abrir um dropdown, nele voc√™ deve clicar na op√ß√£o \"Change runtime type\" (Trocar tipo de ambiente de execu√ß√£o)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0dEASLl5cEuR"
   },
   "source": [
    "![Trocar tipo do Runtime](https://i.imgur.com/XYCBjPZ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SFj23X9dBoQ"
   },
   "source": [
    "Vai abrir um modal, nele voc√™ deve apenas selecionar a op√ß√£o no select de \"Runtime version\" (Vers√µes do ambiente de execu√ß√£o) que tenha o nome \"2025.07\".\n",
    "\n",
    "Olhando na [documenta√ß√£o](https://research.google.com/colaboratory/runtime-version-faq.html#2025.07) voc√™ pode observar que a vers√£o do python √© a que necessitamos (3.11.*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gRwFkmaccbi"
   },
   "source": [
    "![Alterar vers√£o](https://i.imgur.com/kz9XhDD.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1PN4tvRdklq"
   },
   "source": [
    "O motivo da troca de vers√£o √© porque temos depend√™ncias que n√£o foram disponiilizadas para as vers√µes mais recentes do python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4OliaBGcJjg"
   },
   "source": [
    "![]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxZDIu9EQcTR"
   },
   "source": [
    "# Informa√ß√µes sobre o projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWWfNAZgQqCn"
   },
   "source": [
    "## Integrantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15GP3pGPQrz_"
   },
   "source": [
    "Os integrantes desse projeto s√£o:\n",
    "- [Rhogger Freitas Silva](https://www.linkedin.com/in/rhogger-fs/)\n",
    "- [Jos√© Henrique Queiroz de Souza](https://www.linkedin.com/in/josehenriqve/)\n",
    "- [Mateus Abreu da Cunha Nascimento](https://www.linkedin.com/in/mateusabreucn/)\n",
    "- [Felipe Peretti](https://www.linkedin.com/in/felipeperetti/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-q1xjOJQn8t"
   },
   "source": [
    "## Sobre o projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGKqxObtQspJ"
   },
   "source": [
    "Este √© um projeto desenvolvido durante a disciplina \"Projeto de Ci√™ncia de Dados\" da p√≥s-gradua√ß√£o de Data Science & Machine Learning da UniRV.\n",
    "\n",
    "<br>\n",
    "\n",
    "O foco da disciplina √© criar um projeto completo de ci√™ncia de dados para aplicarmos os conhecimentos obtidos ao longo do curso e apresent√°-lo √† uma banca avaliadora composta de profissionais das √°reas na qual foi aplicado o projeto.\n",
    "\n",
    "<br>\n",
    "\n",
    "O objetivo do projeto √© limpar e preparar os dados para extra√ß√£o de insights, preprocessar e normalizar os dados para a gera√ß√£o de modelos na predi√ß√£o se uma determinada a√ß√£o ordin√°ria da bolsa de valores √© recomendado o investimento, e estimar o seu valor em 2025.\n",
    "\n",
    "<br>\n",
    "\n",
    "Fonte dos dados: [Economatica](https://www.economatica.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YIceniDQilR"
   },
   "source": [
    "# Configura√ß√£o de ambientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrPwJvpSB-pI"
   },
   "source": [
    "## Instala√ß√£o de depend√™ncias\n",
    "\n",
    "Essa etapa demora um pouco, por favor aguarde.\n",
    "\n",
    "### üö®üö®üö® Ap√≥s finalizar, reinicie a sess√£o üö®üö®üö®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gbWeRrq6hsq9",
    "outputId": "88368d95-4014-4b2b-85fe-75d2895142a1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Primeiro, tenta carregar as vari√°veis do .env se a biblioteca estiver dispon√≠vel\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "except ImportError:\n",
    "    print(\"python-dotenv n√£o est√° instalado. Tentando verificar atrav√©s de outras formas...\")\n",
    "\n",
    "# Verifica se est√° rodando no Google Colab\n",
    "# Prioriza a vari√°vel GOOGLE_COLAB_RUNTIME do .env, mas tamb√©m verifica 'google.colab' no sys.modules\n",
    "is_colab = False\n",
    "\n",
    "# Primeiro verifica a vari√°vel de ambiente\n",
    "google_colab_env = os.getenv('GOOGLE_COLAB_RUNTIME')\n",
    "print(f\"Valor da vari√°vel GOOGLE_COLAB_RUNTIME: {google_colab_env}\")\n",
    "\n",
    "if google_colab_env and google_colab_env.lower() == 'true':\n",
    "    is_colab = True\n",
    "    print(\"Google Colab detectado atrav√©s da vari√°vel GOOGLE_COLAB_RUNTIME\")\n",
    "else:\n",
    "    # Fallback: verifica se est√° rodando no Google Colab atrav√©s do sys.modules\n",
    "    import sys\n",
    "    if 'google.colab' in sys.modules:\n",
    "        is_colab = True\n",
    "        print(\"Google Colab detectado atrav√©s do m√≥dulo google.colab\")\n",
    "\n",
    "if is_colab:\n",
    "    print(\"Executando no Google Colab - Instalando depend√™ncias...\")\n",
    "    %pip install -qqq python-dotenv\n",
    "    %pip uninstall -qqq numpy pandas pycaret -y\n",
    "    %pip install -qqq numpy pandas pycaret[full] category_encoders\n",
    "else:\n",
    "    print(\"N√£o est√° no Google Colab - Pulando instala√ß√£o de depend√™ncias via pip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYfxojV4R4vz"
   },
   "source": [
    "## Importa√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fpcwsw5RFoHB",
    "outputId": "d0b643f7-c5f0-4b10-f5b7-e80d1b2cf296"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "276f6EmgOFj_",
    "outputId": "8ce04792-d141-4195-951f-a69212cb0122"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import gdown\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats\n",
    "\n",
    "from pycaret.regression import *\n",
    "from pycaret.classification import *\n",
    "import category_encoders as ce\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bOzBOLIUU6Sl"
   },
   "source": [
    "## Download de Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OB2Kj1PIczNq"
   },
   "source": [
    "Nessa etapa realizamos o download dos datasets (em formato csv) e armazenamento local aqui no colab, utilizando a biblioteca `gdown`, para n√£o se fazer necess√°rio o upload manual por cada usu√°rio deste notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYQySKrLdIfQ"
   },
   "source": [
    "Este primeiro dataset, √© o que possui as principais informa√ß√µes que iremos utilizar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "ND0-L1gfVAqz",
    "outputId": "66bc57f7-2633-4c53-b524-2303aed7b5ae"
   },
   "outputs": [],
   "source": [
    "# Criar a pasta datasets se n√£o existir (apenas no ambiente local)\n",
    "if not is_colab:\n",
    "    os.makedirs('../datasets', exist_ok=True)\n",
    "\n",
    "url = 'https://drive.google.com/file/d/1XmkjSrATB20AafqaxJb6dU-4NGllGcPt/view'\n",
    "\n",
    "# Define o caminho baseado no ambiente\n",
    "if is_colab:\n",
    "    output = '/content/df_economatica.csv'\n",
    "else:\n",
    "    output = '../datasets/df_economatica.csv'\n",
    "\n",
    "print(f\"Baixando arquivo para: {output}\")\n",
    "gdown.download(url, output, fuzzy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3v0SHzS1dWV-"
   },
   "source": [
    "J√° este segundo, √© o outro que ficou faltando algumas colunas relevantes para a nossa an√°lise posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "ZqGPbzaGGse3",
    "outputId": "3c5cfe9c-0aa2-4f04-875d-3137bb9bbe59"
   },
   "outputs": [],
   "source": [
    "url = 'https://drive.google.com/file/d/1FyURuDTdL-hVONTWO2y9rTaUfz88Fmyj/view'\n",
    "\n",
    "# Define o caminho baseado no ambiente\n",
    "if is_colab:\n",
    "    output = '/content/df_economaticav2.csv'\n",
    "else:\n",
    "    output = '../datasets/df_economaticav2.csv'\n",
    "\n",
    "print(f\"Baixando arquivo para: {output}\")\n",
    "gdown.download(url, output, fuzzy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snF3lw35EQAt"
   },
   "source": [
    "## Configura√ß√µes Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z39O-MStEZIt"
   },
   "source": [
    "Configura√ß√µes de exibi√ß√£o do pandas atualizadas para mostrar todas as linhas e colunas, pois o dataset possui mais de 100 colunas, logo o pandas trunca as tabelas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CjoLGpKUER7c"
   },
   "outputs": [],
   "source": [
    "if is_colab:\n",
    "  pd.set_option('display.max_rows', None)\n",
    "  pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configura√ß√µes Plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cores corrigidas conforme especificado\n",
    "GRADIO_PRIMARY = \"#F97316\"       # Cor prim√°ria (laranja)\n",
    "GRADIO_SECONDARY = \"#3267BD\"     # Cor secund√°ria (azul)\n",
    "GRADIO_DARK_BG = \"#27272A\"       # Fundo do gr√°fico\n",
    "GRADIO_CARD_BG = \"#27272A\"       # Fundo dos cards/containers (mesmo que o bg do gr√°fico)\n",
    "GRADIO_BORDER = \"#3F3F46\"        # Linhas do gr√°fico/bordas\n",
    "GRADIO_TEXT = \"#E7E7E8\"          # Labels/texto principal\n",
    "GRADIO_TEXT_MUTED = \"#A1A1AA\"    # Texto secund√°rio (derivado dos labels)\n",
    "\n",
    "# Criar tema customizado baseado no gradio dark com cores corretas\n",
    "custom_theme = {\n",
    "    \"layout\": {\n",
    "        \"paper_bgcolor\": GRADIO_DARK_BG,     # Fundo principal do gr√°fico\n",
    "        \"plot_bgcolor\": GRADIO_DARK_BG,      # Fundo da √°rea de plotagem\n",
    "        \"font\": {\"color\": GRADIO_TEXT, \"family\": \"Inter, system-ui, sans-serif\"},\n",
    "        \"colorway\": [GRADIO_PRIMARY, GRADIO_SECONDARY, \"#10b981\", \"#f59e0b\", \"#ef4444\", \"#8b5cf6\", \"#06b6d4\", \"#84cc16\"],\n",
    "        \"title\": {\n",
    "            \"font\": {\"color\": GRADIO_TEXT, \"size\": 18},\n",
    "            \"x\": 0.5,  # Centraliza t√≠tulo\n",
    "            \"xanchor\": \"center\"\n",
    "        },\n",
    "        \"xaxis\": {\n",
    "            \"gridcolor\": GRADIO_BORDER,\n",
    "            \"linecolor\": GRADIO_BORDER,\n",
    "            \"tickcolor\": GRADIO_BORDER,\n",
    "            \"color\": GRADIO_TEXT,\n",
    "            \"zerolinecolor\": GRADIO_BORDER\n",
    "        },\n",
    "        \"yaxis\": {\n",
    "            \"gridcolor\": GRADIO_BORDER, \n",
    "            \"linecolor\": GRADIO_BORDER,\n",
    "            \"tickcolor\": GRADIO_BORDER,\n",
    "            \"color\": GRADIO_TEXT,\n",
    "            \"zerolinecolor\": GRADIO_BORDER\n",
    "        },\n",
    "        \"legend\": {\n",
    "            \"bgcolor\": f\"rgba({int(GRADIO_CARD_BG[1:3], 16)}, {int(GRADIO_CARD_BG[3:5], 16)}, {int(GRADIO_CARD_BG[5:7], 16)}, 0.9)\",\n",
    "            \"bordercolor\": GRADIO_BORDER,\n",
    "            \"font\": {\"color\": GRADIO_TEXT}\n",
    "        },\n",
    "        \"coloraxis\": {\n",
    "            \"colorbar\": {\n",
    "                \"tickcolor\": GRADIO_BORDER,\n",
    "                \"title\": {\"font\": {\"color\": GRADIO_TEXT}}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Registrar o tema customizado\n",
    "pio.templates[\"gradio_dark\"] = go.layout.Template(custom_theme)\n",
    "\n",
    "# Definir como tema padr√£o\n",
    "pio.templates.default = \"gradio_dark\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7q2nx2qbWDMG"
   },
   "source": [
    "## Defini√ß√£o de datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIvYTjJid2xQ"
   },
   "source": [
    "Agora iremos armazenar os valores dos datasets √† vari√°veis do tipo `DataFrame` do `pandas`, no qual iremos utilizaremos ao decorrer do projeto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uq1WcJ5sWFtO"
   },
   "outputs": [],
   "source": [
    "if is_colab:\n",
    "    path_main = '/content/df_economatica.csv'\n",
    "    path_dividendos = '/content/df_economaticav2.csv'\n",
    "else:\n",
    "    path_main = '../datasets/df_economatica.csv'\n",
    "    path_dividendos = '../datasets/df_economaticav2.csv'\n",
    "\n",
    "df_economatica = pd.read_csv(path_main)\n",
    "df_economatica_dividendos = pd.read_csv(path_dividendos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ql_TjyLgjWhz",
    "outputId": "45ec2ea1-557c-4297-f6bb-f98c668aea5d"
   },
   "outputs": [],
   "source": [
    "df_economatica.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFqx7_djjX5P",
    "outputId": "5d374460-331d-4351-a13c-db38deee1f2e"
   },
   "outputs": [],
   "source": [
    "df_economatica.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vj6MAICeT38"
   },
   "source": [
    "Aqui confirmamos a quantidade de linhas do dataset para uma mesclagem dosdatasets, posteriormente. Este valor √© importante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "XcvI7nZgjZMn",
    "outputId": "a4bef7f7-af4b-4318-b80f-d4f1c8b08d1a"
   },
   "outputs": [],
   "source": [
    "df_economatica.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bQLwD535jaj4",
    "outputId": "8a34fc31-3ce0-4713-c5fb-db3c5203cb60"
   },
   "outputs": [],
   "source": [
    "df_economatica_dividendos.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6uNhgjwljcUM",
    "outputId": "789c0583-6f75-4802-b4f8-17c3db12eb35"
   },
   "outputs": [],
   "source": [
    "df_economatica_dividendos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LNwvQqzenai"
   },
   "source": [
    "J√° aqui vemos que n√£o h√° a mesma quantidade de linhas. Ou seja, iremos analisar e prorrogar a mesclagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "7RUxVBrqjdTY",
    "outputId": "f4bb77a8-3a69-4612-e988-44cf376868fc"
   },
   "outputs": [],
   "source": [
    "df_economatica_dividendos.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTt1CgaxjiHy"
   },
   "source": [
    "# Manipula√ß√£o de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDgTAPYbJ55h"
   },
   "source": [
    "## Filtrando a classe por a√ß√µes ordin√°rias (ON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78iUOmOdDfH9",
    "outputId": "1856e54c-9c02-430f-d0a8-52b412143a6d"
   },
   "outputs": [],
   "source": [
    "df_economatica_on = df_economatica[df_economatica['Classe'] == 'ON']\n",
    "\n",
    "df_economatica_on.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nv9NS49oh6MQ",
    "outputId": "4dea913f-b0a1-4003-9fed-d293a9ca77ac"
   },
   "outputs": [],
   "source": [
    "df_economatica_dividendos_on = df_economatica_dividendos[df_economatica_dividendos['Classe'] == 'ON']\n",
    "\n",
    "df_economatica_dividendos_on.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g_8yOxlojp-X"
   },
   "source": [
    "## Remo√ß√£o de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xQuoHkIkZsL"
   },
   "source": [
    "Removendo as colunas \"Classe\", \"Bolsa / Fonte\", \"Tipo de Ativo\", \"Ativo / Cancelado\" e \"Setor Agro Bovespa\", pois todas essas colunas possu√≠am o mesmo valor para todas as linhas (a√ß√µes), sendo \"ON\", \"Bovespa\", \"A√ß√£o\" e \"ativo\", respectivamente, esses valores n√£o ser√£o relevantes a partir de agora e todos possuem o mesmo valor.\n",
    "\n",
    "<br>\n",
    "\n",
    "A justificativa de remover a coluna \"Setor Agro Bovespa\" se d√° pelo fato de que iremos realizar o OneHotEndcoding mais para frente, logo √© mais simples remover a coluna do que normalizar os valores.\n",
    "\n",
    "<br>\n",
    "\n",
    "As colunas de ativos/passivos circulantes e n√£o circulantes, s√£o os componentes que formam valores maiores, como o ativo_total e o passivo_total. A informa√ß√£o contida nelas j√° est√° representada de forma mais consolidada e √∫til nos pr√≥prios totais e, principalmente, nos indicadores de liquidez e endividamento. Mant√™-las no modelo criaria uma redund√¢ncia desnecess√°ria. Por√©m elas ser√£o removidas posteriormente, pois ser√£o utilizadas para c√°lculos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hjwwlc-7jCp6",
    "outputId": "260c2a38-e80c-4a01-be8d-1805fedde541"
   },
   "outputs": [],
   "source": [
    "colunas_para_remover_df_economatica = [\n",
    "    'Classe',\n",
    "    'Bolsa / Fonte',\n",
    "    'Tipo de Ativo',\n",
    "    'Ativo /\\nCancelado',\n",
    "    'Setor Agro\\nBovespa',\n",
    "    # 'AtvCir\\n < Dez 2020\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'AtvCir\\n < Dez 2021\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'AtvCir\\n < Dez 2022\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'AtvCir\\n < Dez 2023\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'AtvCir\\n < Dez 2024\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'AtvNaoCir\\n < Dez 2020\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'AtvNaoCir\\n < Dez 2021\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'AtvNaoCir\\n < Dez 2022\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'AtvNaoCir\\n < Dez 2023\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'AtvNaoCir\\n < Dez 2024\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'PasCir\\n < Dez 2020\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'PasCir\\n < Set 2021\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'PasCir\\n < Dez 2022\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'PasCir\\n < Dez 2023\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'PasCir\\n < Dez 2024\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'PasNoCir\\n < Dez 2020\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'PasNoCir\\n < Dez 2021\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'PasNoCir\\n < Dez 2022\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'PasNoCir\\n < Dez 2023\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'PasNoCir\\n < Dez 2024\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'Ativo Tot\\n < Dez 2020\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'Ativo Tot\\n < Dez 2021\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'Ativo Tot\\n < Dez 2022\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'Ativo Tot\\n < Dez 2023\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'Ativo Tot\\n < Dez 2024\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'Patrim Liq\\n < Dez 2020\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'Patrim Liq\\n < Dez 2021\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'Patrim Liq\\n < Dez 2022\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'Patrim Liq\\n < Dez 2023\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'Patrim Liq\\n < Dez 2024\\n Em moeda orig\\n em milhares\\n consolid:sim*',\n",
    "    # 'Receita\\n < Dez 2020\\n Em moeda orig\\n em milhares\\n no exerc√≠cio\\n consolid:sim*',\n",
    "    # 'Receita\\n < Dez 2021\\n Em moeda orig\\n em milhares\\n no exerc√≠cio\\n consolid:sim*',\n",
    "    # 'Receita\\n < Dez 2022\\n Em moeda orig\\n em milhares\\n no exerc√≠cio\\n consolid:sim*',\n",
    "    # 'Receita\\n < Dez 2023\\n Em moeda orig\\n em milhares\\n no exerc√≠cio\\n consolid:sim*',\n",
    "    # 'Receita\\n < Dez 2024\\n Em moeda orig\\n em milhares\\n no exerc√≠cio\\n consolid:sim*',\n",
    "    # 'Lucro Liquido\\n < Dez 2020\\n Em moeda orig\\n em milhares\\n no exerc√≠cio\\n consolid:sim*',\n",
    "    # 'Lucro Liquido\\n < Dez 2021\\n Em moeda orig\\n em milhares\\n no exerc√≠cio\\n consolid:sim*',\n",
    "    # 'Lucro Liquido\\n < Dez 2022\\n Em moeda orig\\n em milhares\\n no exerc√≠cio\\n consolid:sim*',\n",
    "    # 'Lucro Liquido\\n < Dez 2023\\n Em moeda orig\\n em milhares\\n no exerc√≠cio\\n consolid:sim*',\n",
    "    # 'Lucro Liquido\\n < Dez 2024\\n Em moeda orig\\n em milhares\\n no exerc√≠cio\\n consolid:sim*',\n",
    "]\n",
    "\n",
    "df_economatica_colunas_removidas = df_economatica_on.drop(columns=colunas_para_remover_df_economatica).copy()\n",
    "\n",
    "df_economatica_colunas_removidas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IOsuKudKjSN7",
    "outputId": "83dabd1c-e574-4764-ff85-fd8fcd6f0588"
   },
   "outputs": [],
   "source": [
    "colunas_para_remover_df_economatica_dividendos = [\n",
    "    'Classe',\n",
    "    'Bolsa / Fonte',\n",
    "    'Tipo de Ativo',\n",
    "    'Ativo /\\nCancelado'\n",
    "]\n",
    "\n",
    "df_economatica_dividendos_colunas_removidas = df_economatica_dividendos_on.drop(columns=colunas_para_remover_df_economatica_dividendos)\n",
    "\n",
    "df_economatica_dividendos_colunas_removidas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uq2le294Ouhf"
   },
   "source": [
    "## Renomeando as colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cMnCTKeBX17U",
    "outputId": "08278d1b-fb9e-4657-9b5a-2e735252e9d9"
   },
   "outputs": [],
   "source": [
    "novos_nomes_colunas_df_economatica = [\n",
    "    'nome', 'codigo', 'setor',\n",
    "    'ativo_circulante_2020', 'ativo_circulante_2021', 'ativo_circulante_2022', 'ativo_circulante_2023', 'ativo_circulante_2024',\n",
    "    'ativo_nao_circulante_2020', 'ativo_nao_circulante_2021', 'ativo_nao_circulante_2022', 'ativo_nao_circulante_2023', 'ativo_nao_circulante_2024',\n",
    "    'passivo_circulante_2020', 'passivo_circulante_2021', 'passivo_circulante_2022', 'passivo_circulante_2023', 'passivo_circulante_2024',\n",
    "    'passivo_nao_circulante_2020', 'passivo_nao_circulante_2021', 'passivo_nao_circulante_2022', 'passivo_nao_circulante_2023', 'passivo_nao_circulante_2024',\n",
    "    'patrimonio_liquido_2020', 'patrimonio_liquido_2021', 'patrimonio_liquido_2022', 'patrimonio_liquido_2023', 'patrimonio_liquido_2024',\n",
    "    'lpa_2020', 'lpa_2021', 'lpa_2022', 'lpa_2023', 'lpa_2024',\n",
    "    'receita_2020', 'receita_2021', 'receita_2022', 'receita_2023', 'receita_2024',\n",
    "    'lucro_liquido_2020', 'lucro_liquido_2021', 'lucro_liquido_2022', 'lucro_liquido_2023', 'lucro_liquido_2024',\n",
    "    'divida_bruta_ativo_2020', 'divida_bruta_ativo_2021', 'divida_bruta_ativo_2022', 'divida_bruta_ativo_2023', 'divida_bruta_ativo_2024',\n",
    "    'liquidez_corrente_2020', 'liquidez_corrente_2021', 'liquidez_corrente_2022', 'liquidez_corrente_2023', 'liquidez_corrente_2024',\n",
    "    'liquidez_geral_2020', 'liquidez_geral_2021', 'liquidez_geral_2022', 'liquidez_geral_2023', 'liquidez_geral_2024',\n",
    "    'rentabilidade_ativo_2020', 'rentabilidade_ativo_2021', 'rentabilidade_ativo_2022', 'rentabilidade_ativo_2023', 'rentabilidade_ativo_2024',\n",
    "    'roe_2020', 'roe_2021', 'roe_2022', 'roe_2023', 'roe_2024',\n",
    "    'margem_ebit_2020', 'margem_ebit_2021', 'margem_ebit_2022', 'margem_ebit_2023', 'margem_ebit_2024',\n",
    "    'margem_liquida_2020', 'margem_liquida_2021', 'margem_liquida_2022', 'margem_liquida_2023', 'margem_liquida_2024',\n",
    "    'ativo_total_2020', 'ativo_total_2021', 'ativo_total_2022', 'ativo_total_2023', 'ativo_total_2024',\n",
    "    'caixa_operacional_2020', 'caixa_operacional_2021', 'caixa_operacional_2022', 'caixa_operacional_2023', 'caixa_operacional_2024',\n",
    "    'caixa_investimento_2020', 'caixa_investimento_2021', 'caixa_investimento_2022', 'caixa_investimento_2023', 'caixa_investimento_2024',\n",
    "    'caixa_financeiro_2020', 'caixa_financeiro_2021', 'caixa_financeiro_2022', 'caixa_financeiro_2023', 'caixa_financeiro_2024',\n",
    "    'valor_mercado_2020', 'valor_mercado_2021', 'valor_mercado_2022', 'valor_mercado_2023', 'valor_mercado_2024'\n",
    "]\n",
    "\n",
    "df_economatica_renomeado = df_economatica_colunas_removidas.copy()\n",
    "df_economatica_renomeado.columns = novos_nomes_colunas_df_economatica\n",
    "\n",
    "df_economatica_renomeado.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5MwkjSF0kkqx",
    "outputId": "09d5287b-441f-416d-c372-122464f3f6f2"
   },
   "outputs": [],
   "source": [
    "novos_nomes_colunas_df_economatica_dividendos = [\n",
    "    'nome', 'codigo', 'dividendos_2020', 'dividendos_2021', 'dividendos_2022', 'dividendos_2023', 'dividendos_2024'\n",
    "]\n",
    "\n",
    "df_economatica_dividendos_renomeado = df_economatica_dividendos_colunas_removidas.copy()\n",
    "df_economatica_dividendos_renomeado.columns = novos_nomes_colunas_df_economatica_dividendos\n",
    "\n",
    "df_economatica_dividendos_renomeado.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPE2pN50miU5"
   },
   "source": [
    "## Verificando integridade dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEdzUzrKmoz-"
   },
   "source": [
    "Antes de criar as novas features, devemos mesclar os dataframes, mas como vimos anteriormente, a quantidade de linhas est√° diferente, e √© isso que iremos analisar neste bloco.\n",
    "\n",
    "Para fazer isso, basta verificar qual c√≥digo n√£o est√° presente no dataset dos dividendos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "id": "VHRDpBQLmoWM",
    "outputId": "79fab7f4-0e33-4b1f-b9cf-c244ac7b3b2a"
   },
   "outputs": [],
   "source": [
    "codigos_ausentes_em_economatica_limpo = df_economatica_dividendos_renomeado[\n",
    "    ~df_economatica_dividendos_renomeado['codigo'].isin(df_economatica_renomeado['codigo'])\n",
    "]['codigo']\n",
    "\n",
    "print(f\"Foi encontrado {codigos_ausentes_em_economatica_limpo.count()} c√≥digos presentes em df_economatica_dividendos_renomeado, mas ausentes em df_economatica_renomeado:\")\n",
    "codigos_ausentes_em_economatica_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "PCeH7AvsqMfx",
    "outputId": "bdb04c32-e080-4f58-d4fb-9879389de113"
   },
   "outputs": [],
   "source": [
    "codigos_ausentes_em_dividendos_limpo = df_economatica_renomeado[\n",
    "    ~df_economatica_renomeado['codigo'].isin(df_economatica_dividendos_renomeado['codigo'])\n",
    "]['codigo']\n",
    "\n",
    "print(f\"Foi encontrado {codigos_ausentes_em_dividendos_limpo.count()} c√≥digos presentes em df_economatica_renomeado, mas ausentes em df_economatica_dividendos_renomeado:\")\n",
    "codigos_ausentes_em_dividendos_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "CkJCkPoKqWvr",
    "outputId": "7b584eed-f25c-46c9-d093-6e4aee378e9e"
   },
   "outputs": [],
   "source": [
    "nomes_ausentes_em_economatica_limpo = df_economatica_dividendos_renomeado[\n",
    "    ~df_economatica_dividendos_renomeado['nome'].isin(df_economatica_renomeado['nome'])\n",
    "]['nome']\n",
    "\n",
    "print(f\"Foi encontrado {nomes_ausentes_em_economatica_limpo.count()} nomes de empresas presentes em df_economatica_dividendos_renomeado, mas ausentes em df_economatica_renomeado:\")\n",
    "nomes_ausentes_em_economatica_limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "id": "yCTDXzyAq1p3",
    "outputId": "308d4313-602f-4c52-ddf4-786b9e350653"
   },
   "outputs": [],
   "source": [
    "nomes_ausentes_em_dividendos_limpo = df_economatica_renomeado[\n",
    "    ~df_economatica_renomeado['nome'].isin(df_economatica_dividendos_renomeado['nome'])\n",
    "]['nome']\n",
    "\n",
    "print(f\"\\n Foi encontrado {nomes_ausentes_em_dividendos_limpo.count()} nomes de empresas presentes em df_economatica_renomeado, mas ausentes em df_economatica_dividendos_renomeado:\")\n",
    "nomes_ausentes_em_dividendos_limpo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUL9RLPbxupu"
   },
   "source": [
    "Verificado os valores inconsistentes, iremos remov√™-los."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W4wmE4cTyL6-",
    "outputId": "09833b46-e205-4120-8fcb-41fcbb4b3c92"
   },
   "outputs": [],
   "source": [
    "df_economatica_filtrado = df_economatica_renomeado[\n",
    "    df_economatica_renomeado['codigo'].isin(df_economatica_dividendos_renomeado['codigo'])\n",
    "].copy()\n",
    "\n",
    "print(f\"Shape de df_economatica_renomeado ap√≥s remo√ß√£o: {df_economatica_filtrado.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MFq3QIaVyhwO",
    "outputId": "499ed631-00ae-4310-cb56-dd56c2de50f7"
   },
   "outputs": [],
   "source": [
    "df_economatica_dividendos_filtrado = df_economatica_dividendos_renomeado[\n",
    "    df_economatica_dividendos_renomeado['codigo'].isin(df_economatica_renomeado['codigo'])\n",
    "].copy()\n",
    "\n",
    "print(f\"Shape de df_economatica_dividendos_renomeado ap√≥s remo√ß√£o: {df_economatica_dividendos_filtrado.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oOnz4w5PYNzh"
   },
   "source": [
    "## Mesclagem dos datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhXJXKAwy29j"
   },
   "source": [
    "Agora que temos os datasets com os mesmos registros, iremos mescl√°-los."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gWE9HRx_YT_q",
    "outputId": "51cea2f8-b76b-4d9b-eab5-4515d3f3e914"
   },
   "outputs": [],
   "source": [
    "colunas_dividendos_para_mesclar = [col for col in df_economatica_dividendos_filtrado.columns if col not in df_economatica_filtrado.columns or col == 'codigo']\n",
    "\n",
    "df_economatica_mesclado = pd.merge(\n",
    "    df_economatica_filtrado,\n",
    "    df_economatica_dividendos_filtrado[colunas_dividendos_para_mesclar],\n",
    "    on='codigo',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"Shape do DataFrame mesclado:\")\n",
    "print(df_economatica_mesclado.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s5RNiDy-Yhnd",
    "outputId": "8c6a2762-7bc4-458f-85d4-675a9c3c5324"
   },
   "outputs": [],
   "source": [
    "df_economatica_mesclado.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gOOnZCQJYnN7",
    "outputId": "7362fa9c-5ca9-457b-a287-2e497709272c"
   },
   "outputs": [],
   "source": [
    "df_economatica_mesclado.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5W8jfdG9YuUB"
   },
   "source": [
    "Procurando atras do regex '_\\d{4}$' todas as colunas que terminam com _ano\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SbJME-OUlkI"
   },
   "outputs": [],
   "source": [
    "colunas_com_ano = [col for col in df_economatica_mesclado.columns if re.search(r'_\\d{4}$', col)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAcPjnVuY8vk"
   },
   "source": [
    "Dividindo a base das colunas por ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPsIxZW5W12K"
   },
   "outputs": [],
   "source": [
    "base_colunas = defaultdict(list)\n",
    "for col in colunas_com_ano:\n",
    "    match = re.match(r\"(.*)_(\\d{4})$\", col)\n",
    "    if match:\n",
    "        base, ano = match.groups()\n",
    "        base_colunas[base].append(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jD9vfAxEZYwj"
   },
   "source": [
    "Fazendo o melt (extens√£o dos dados) com o ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4aYr8ujWwP1"
   },
   "outputs": [],
   "source": [
    "dfs_long = []\n",
    "id_vars = [col for col in df_economatica_mesclado.columns if col not in colunas_com_ano]  # colunas que voc√™ quer manter (ex: nome, setor, etc.)\n",
    "\n",
    "for base, cols in base_colunas.items():\n",
    "    # melt para cada base\n",
    "    df_long = df_economatica_mesclado.melt(id_vars=id_vars, value_vars=cols,\n",
    "                      var_name='variavel', value_name=base)\n",
    "    # extrai o ano\n",
    "    df_long['ano'] = df_long['variavel'].str.extract(r'(\\d{4})$')\n",
    "    # drop coluna tempor√°ria\n",
    "    df_long = df_long.drop(columns=['variavel'])\n",
    "    dfs_long.append(df_long)\n",
    "\n",
    "# Agora voc√™ pode juntar tudo pelo id_vars + ano\n",
    "from functools import reduce\n",
    "\n",
    "df_economatica_com_ano = reduce(lambda left, right: pd.merge(left, right, on=id_vars + ['ano'], how='outer'), dfs_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "i1amKWvLXILz",
    "outputId": "ba79e816-d93a-4c31-ab5b-df6a8dda7293"
   },
   "outputs": [],
   "source": [
    "df_economatica_com_ano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_PR2ip6jruB"
   },
   "source": [
    "## Cria√ß√£o de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYN1XrXt09TL"
   },
   "source": [
    "### Feature - ano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iwr7x_0eSRWt"
   },
   "source": [
    "Agora iremos dividir em 5 datasets, sendo cada um as a√ß√µes de um ano espec√≠fico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6wRjgxk4GFsB"
   },
   "outputs": [],
   "source": [
    "def criar_df_ano(ano: int):\n",
    "  df_ano = df_economatica_mesclado[['nome', 'codigo', 'setor'] + [col for col in df_economatica_mesclado.columns if col.endswith(f'_{ano}')]].copy()\n",
    "  df_ano.columns = ['nome', 'codigo', 'setor'] + [col.replace(f'_{ano}', '') for col in df_ano.columns if col.endswith(f'_{ano}')]\n",
    "  df_ano['ano'] = ano\n",
    "\n",
    "  print(f\"Shape do DataFrame de {ano}:\", df_ano.shape)\n",
    "\n",
    "  return df_ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9fLSIrd1D7g",
    "outputId": "bb11a08c-9760-49b2-a25d-faa71be1ba84"
   },
   "outputs": [],
   "source": [
    "df_economatica_2020 = criar_df_ano(2020)\n",
    "df_economatica_2021 = criar_df_ano(2021)\n",
    "df_economatica_2022 = criar_df_ano(2022)\n",
    "df_economatica_2023 = criar_df_ano(2023)\n",
    "df_economatica_2024 = criar_df_ano(2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_anos = [\n",
    "    df_economatica_2020,\n",
    "    df_economatica_2021, \n",
    "    df_economatica_2022,\n",
    "    df_economatica_2023,\n",
    "    df_economatica_2024\n",
    "]\n",
    "\n",
    "# Concatenar todos os DataFrames\n",
    "df_economatica_com_ano = pd.concat(dataframes_anos, ignore_index=True)\n",
    "\n",
    "print(f\"Shape do DataFrame mesclado: {df_economatica_com_ano.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ji1e4EoBfEbh"
   },
   "source": [
    "### Feature - pandemia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thB2-4BWfIwz"
   },
   "source": [
    "Neste trecho iremos adicionar 1 na coluna pandemia nos anos que estavam em alerta emergencial de COVID-19, segundo a OMS, e 0 nos que nao estavam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dQv5OPFKfLxn",
    "outputId": "c2439401-dfe5-43bf-bcee-6bd264a261c5"
   },
   "outputs": [],
   "source": [
    "df_economatica_manipulado = df_economatica_com_ano.copy()\n",
    "\n",
    "df_economatica_manipulado['pandemia'] = df_economatica_com_ano['ano'].apply(\n",
    "    lambda x: 1 if x in [2020, 2021, 2022] else 0\n",
    ")\n",
    "\n",
    "print(\"Coluna 'pandemia' criada com sucesso!\")\n",
    "print(f\"Shape do DataFrame: {df_economatica_manipulado.shape}\")\n",
    "print(\"\\nDistribui√ß√£o da vari√°vel pandemia:\")\n",
    "print(df_economatica_manipulado['pandemia'].value_counts())\n",
    "print(\"\\nDistribui√ß√£o por ano e pandemia:\")\n",
    "print(df_economatica_manipulado.groupby(['ano', 'pandemia']).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature - variacao_valor_mercado (Vari√°vel Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_economatica_manipulado['variacao_valor_mercado'] = np.nan\n",
    "\n",
    "df_economatica_manipulado['variacao_valor_mercado'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRnKAVzAJ2do"
   },
   "source": [
    "# Limpeza dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elOF-fQ1e21z"
   },
   "source": [
    "Antes e come√ßar a utilizar os dados, precisamos realizar algumas etapas de  limpeza dos dados, como:\n",
    "- Verificar e tratar dados duplicados.\n",
    "- Verificar e tratar tipagem.\n",
    "- Verificar e tratar nulos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2RTCRoUwlf-o"
   },
   "source": [
    "## Tratamento de valores duplicados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "61oc9XSelihH",
    "outputId": "98c3cab6-97f7-4fd2-9b7f-2fc61e098270"
   },
   "outputs": [],
   "source": [
    "print(f\"N√∫mero de linhas duplicadas: {df_economatica_manipulado.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nb0GThYVl9s6"
   },
   "source": [
    "Mesmo que n√£o tenha linhas com todos os valores iguais, irei verificar se n√£o h√° c√≥digos duplicados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "Pmqn5gmxmLMn",
    "outputId": "93d71c2f-c9c3-4b94-c800-03dea4107ab7"
   },
   "outputs": [],
   "source": [
    "print(f\"N√∫mero de c√≥digos duplicados: {df_economatica_manipulado['codigo'].duplicated().sum()}\")\n",
    "\n",
    "print(f\"N√∫mero de registros duplicados (mesmo c√≥digo no mesmo ano): {df_economatica_manipulado.duplicated(subset=['codigo', 'ano']).sum()}\")\n",
    "\n",
    "contagem_por_codigo = df_economatica_manipulado['codigo'].value_counts()\n",
    "print(f\"\\nC√≥digos que aparecem 5 vezes (completos): {(contagem_por_codigo == 5).sum()}\")\n",
    "print(f\"C√≥digos que aparecem menos de 5 vezes (incompletos): {(contagem_por_codigo < 5).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ft5_GXnVmbdO"
   },
   "source": [
    "Show! Agora sabemos que nenhum registro √© duplicado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1vuagOXPCJwc"
   },
   "source": [
    "## Verificar e tratar tipagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos iniciar filtrando somente as colunas num√©ricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_numericas = [col for col in df_economatica_manipulado.columns if col not in ['nome', 'codigo', 'setor']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificando formato dos valores inconsistentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checagem_de_valores_nao_numericos(df, columns):\n",
    "    valores_nao_numericos = {}\n",
    "    for col in columns:\n",
    "        # Tentar converter a coluna para num√©rico, com errors='coerce' para transformar valores inv√°lidos em NaN\n",
    "        # Usamos errors='coerce' aqui apenas para identificar o que N√ÉO √© num√©rico, sem modificar o DataFrame original\n",
    "        col_numerica = pd.to_numeric(df[col], errors='coerce')\n",
    "        # Encontrar os valores no DataFrame original que se tornaram NaN ap√≥s a coer√ß√£o e que n√£o eram NaN originalmente\n",
    "        valores_invalidos = df[col][col_numerica.isnull() & df[col].notnull()]\n",
    "        if not valores_invalidos.empty:\n",
    "            valores_nao_numericos[col] = valores_invalidos.unique().tolist()\n",
    "    return valores_nao_numericos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exibir_valores_nao_numericos(df_economatica_mesclado, colunas_numericas):\n",
    "    valores_nao_numericos_encontrados = checagem_de_valores_nao_numericos(df_economatica_mesclado, colunas_numericas)\n",
    "\n",
    "    if valores_nao_numericos_encontrados:\n",
    "        print(\"Valores n√£o num√©ricos encontrados nas seguintes colunas:\")\n",
    "        for col, val in valores_nao_numericos_encontrados.items():\n",
    "            print(f\"- {col}: {val}\")\n",
    "    else:\n",
    "        print(\"Nenhum valor n√£o num√©rico encontrado nas colunas a verificar.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exibir_valores_nao_numericos(df_economatica_manipulado, colunas_numericas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifiquei padr√µes:\n",
    "- Valores nulos representados por: '-'\n",
    "- Valores com casas decimais representados por: '#,##########'\n",
    "- Valores com casas de milhar em diante representados por: '#.###.###'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratando a inconsist√™ncia dos tipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpar_e_converter(value, decimais=4):\n",
    "    \"\"\"\n",
    "    Converte um valor dinamicamente identificando se √© inteiro, float ou string.\n",
    "    Trata pontua√ß√£o brasileira (. para milhares, , para decimais).\n",
    "    \n",
    "    Args:\n",
    "        value: O valor a ser convertido\n",
    "        decimais: N√∫mero de casas decimais para arredondamento (padr√£o: 4)\n",
    "        \n",
    "    Returns:\n",
    "        int, float ou np.nan dependendo do tipo identificado\n",
    "    \"\"\"\n",
    "    # Se j√° √© num√©rico, retorna como est√°\n",
    "    if isinstance(value, (int, float)):\n",
    "        if isinstance(value, float):\n",
    "            return round(value, decimais)\n",
    "        return value\n",
    "        \n",
    "    # Se n√£o √© string, tenta converter para string\n",
    "    if not isinstance(value, str):\n",
    "        value = str(value)\n",
    "    \n",
    "    # Remove espa√ßos e verifica se √© valor nulo\n",
    "    value = value.strip()\n",
    "    if value == '-' or value == '' or value.lower() in ['nan', 'null', 'none']:\n",
    "        return np.nan\n",
    "    \n",
    "    # Remove pontos (separador de milhares) e substitui v√≠rgula por ponto (decimal)\n",
    "    # Primeiro identifica o padr√£o: se tem v√≠rgula, ela √© o separador decimal\n",
    "    if ',' in value:\n",
    "        # Padr√£o brasileiro: 1.234.567,89\n",
    "        # Remove todos os pontos (milhares) e substitui v√≠rgula por ponto (decimal)\n",
    "        value_clean = value.replace('.', '').replace(',', '.')\n",
    "    else:\n",
    "        # Se n√£o tem v√≠rgula, assume padr√£o americano ou n√∫mero sem decimais\n",
    "        # Se tem apenas um ponto e d√≠gitos depois dele <= 3, pode ser decimal\n",
    "        # Se tem mais de um ponto ou mais de 3 d√≠gitos ap√≥s o √∫ltimo ponto, s√£o milhares\n",
    "        parts = value.split('.')\n",
    "        if len(parts) == 2 and len(parts[1]) <= 3 and len(parts[1]) > 0:\n",
    "            # Provavelmente decimal: 1234.56\n",
    "            value_clean = value\n",
    "        elif len(parts) > 2:\n",
    "            # M√∫ltiplos pontos: separadores de milhares: 1.234.567\n",
    "            value_clean = value.replace('.', '')\n",
    "        else:\n",
    "            # Um ponto com mais de 3 d√≠gitos ou sem ponto\n",
    "            value_clean = value.replace('.', '')\n",
    "    \n",
    "    # Remove outros caracteres n√£o num√©ricos (exceto - no in√≠cio)\n",
    "    # Preserva sinal negativo no in√≠cio\n",
    "    is_negative = value_clean.startswith('-')\n",
    "    value_clean = re.sub(r'[^\\d.]', '', value_clean.lstrip('-'))\n",
    "    if is_negative:\n",
    "        value_clean = '-' + value_clean\n",
    "    \n",
    "    try:\n",
    "        # Tenta converter para float primeiro\n",
    "        float_value = float(value_clean)\n",
    "        \n",
    "        # Verifica se √© um inteiro (sem parte decimal)\n",
    "        if float_value.is_integer():\n",
    "            return int(float_value)\n",
    "        else:\n",
    "            return round(float_value, decimais)\n",
    "            \n",
    "    except (ValueError, TypeError):\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_conversao(df, decimais=4):\n",
    "    \"\"\"\n",
    "    Aplica a convers√£o din√¢mica para as colunas especificadas.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame a ser processado\n",
    "        decimais: N√∫mero de casas decimais para arredondamento (padr√£o: 4)\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame com as colunas convertidas\n",
    "    \"\"\"\n",
    "    df_convertido = df.copy()\n",
    "    \n",
    "    for col in colunas_numericas:\n",
    "        if col in df_convertido.columns:\n",
    "            print(f\"Convertendo coluna: {col}\")\n",
    "            df_convertido[col] = df_convertido[col].apply(lambda x: limpar_e_converter(x, decimais))\n",
    "    \n",
    "    return df_convertido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de tratar, vamos criar uma c√≥pia para podermos comparar o come√ßo e o fim de cada dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_economatica_correcao_tipagem = aplicar_conversao(df_economatica_manipulado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplos de uso com diferentes casas decimais\n",
    "\n",
    "Agora voc√™ pode controlar o n√∫mero de casas decimais passando o par√¢metro `decimais`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo: Aplicar convers√£o com 2 casas decimais\n",
    "print(\"=== Exemplo com 2 casas decimais ===\")\n",
    "df_2_decimais = aplicar_conversao(df_economatica_manipulado, decimais=2)\n",
    "\n",
    "# Exemplo: Aplicar convers√£o com 4 casas decimais (padr√£o)\n",
    "print(\"\\n=== Exemplo com 4 casas decimais (padr√£o) ===\")\n",
    "df_4_decimais = aplicar_conversao(df_economatica_manipulado, decimais=4)\n",
    "\n",
    "# Exemplo: Teste individual da fun√ß√£o\n",
    "print(\"\\n=== Testes individuais da fun√ß√£o ===\")\n",
    "print(f\"limpar_e_converter('1.234,567', decimais=2) = {limpar_e_converter('1.234,567', decimais=2)}\")\n",
    "print(f\"limpar_e_converter('1.234,567', decimais=4) = {limpar_e_converter('1.234,567', decimais=4)}\")\n",
    "print(f\"limpar_e_converter('123.45', decimais=2) = {limpar_e_converter('123.45', decimais=2)}\")\n",
    "print(f\"limpar_e_converter('123.45', decimais=0) = {limpar_e_converter('123.45', decimais=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTipagem das colunas ap√≥s tratamento:\")\n",
    "df_economatica_correcao_tipagem.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_economatica_correcao_tipagem.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tratamento de nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SSuYCMnrCSjy"
   },
   "outputs": [],
   "source": [
    "def contar_e_exibir_nulos(df):\n",
    "    \"\"\"\n",
    "    Conta os valores nulos por coluna em um DataFrame e exibe as colunas com nulos.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): O DataFrame de entrada.\n",
    "    \"\"\"\n",
    "    null_counts = df.isnull().sum()\n",
    "    columns_with_nulls = null_counts[null_counts > 0]\n",
    "\n",
    "    print(\"Contagem de valores nulos por coluna (apenas colunas com nulos):\")\n",
    "    print(columns_with_nulls)\n",
    "    print(f'\\n{columns_with_nulls.sum()} c√©lulas possuem valores nulos no total.')\n",
    "    num_linhas_com_nulos = df.isnull().any(axis=1).sum()\n",
    "    print(f\"N√∫mero de linhas com pelo menos um valor nulo: {num_linhas_com_nulos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDBeXJt4-nLd",
    "outputId": "0b9c90e2-96c7-4575-8c2d-355f7c893711"
   },
   "outputs": [],
   "source": [
    "contar_e_exibir_nulos(df_economatica_correcao_tipagem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFuoJAowyqiH"
   },
   "source": [
    "S√£o muitas c√©lulas com valores nulos... Tentaremos adotar as seguintes abordagens:\n",
    "\n",
    "- criar uma matriz de correla√ß√£o para eliminar vari√°veis altamente correlacionadas;\n",
    "- coletar as colunas derivadas a partir de c√°lculos de outras colunas e realizar os c√°lculos (mas para isso precisamos verificar antes se essa coluna utilizada no c√°lculo possui valor);\n",
    "- substituir valores nulos com base na m√©dia da linha (ex: ativo_circulante de 2020 at√© de 2023 possuem valores, mas 2024 n√£o, logo coleto a m√©dia dos 4 anos para gerar o valor em 2024);\n",
    "- e por fim, eliminar as a√ß√µes cujas linhas possuem valores nulos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KV4Cd5Qvhm7"
   },
   "source": [
    "### Matriz de Correla√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kX7Ga_XpAMHG"
   },
   "source": [
    "Nessa etapa iremos selecionar as features que causam a multicolinearidade e remover a que tiver menos correla√ß√£o com a vari√°vel target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EzUaPnN0AjZ9"
   },
   "source": [
    "Primeiro selecionamos somente os valores n√∫mericos.\n",
    "\n",
    "Depois criamos uma fun√ß√£o para calcular a correla√ß√£o e exibir um mapa de calor conforme o ano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iouKQWuY_MEt"
   },
   "outputs": [],
   "source": [
    "df_numerico = df_economatica_correcao_tipagem[colunas_numericas].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sC48I-akvsGD"
   },
   "outputs": [],
   "source": [
    "def calcular_e_exibir_correlacao(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Calcula e exibe a matriz de correla√ß√£o para todas as colunas num√©ricas.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame com os dados\n",
    "    \"\"\"\n",
    "    # Seleciona apenas colunas num√©ricas (excluindo colunas de identifica√ß√£o)\n",
    "    colunas_numericas_filtradas = [col for col in df.columns \n",
    "                                 if col not in ['nome', 'codigo', 'setor'] \n",
    "                                 and df[col].dtype in ['int64', 'float64', 'int32', 'float32']]\n",
    "    \n",
    "    if not colunas_numericas_filtradas:\n",
    "        print(\"Nenhuma coluna num√©rica encontrada\")\n",
    "        return\n",
    "    \n",
    "    # Seleciona apenas as colunas num√©ricas\n",
    "    df_numerico = df[colunas_numericas_filtradas]\n",
    "    \n",
    "    # Remove linhas com muitos NaN para melhorar a correla√ß√£o\n",
    "    df_numerico_limpo = df_numerico.dropna(thresh=len(colunas_numericas_filtradas)*0.5)\n",
    "    \n",
    "    # Calcula a matriz de correla√ß√£o\n",
    "    matriz_correlacao = df_numerico_limpo.corr()\n",
    "    \n",
    "    # Cria uma m√°scara para mostrar apenas o tri√¢ngulo superior\n",
    "    mask = np.triu(np.ones_like(matriz_correlacao, dtype=bool))\n",
    "    \n",
    "    plt.figure(figsize=(20, 18))\n",
    "    sns.heatmap(matriz_correlacao, \n",
    "                mask=mask,\n",
    "                annot=True, \n",
    "                cmap=sns.cubehelix_palette(as_cmap=True),\n",
    "                fmt='.2f',\n",
    "                vmin=-1,\n",
    "                vmax=1,\n",
    "                linewidths=0.1)\n",
    "    plt.title(f'Matriz de Correla√ß√£o')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_correlacao_com_receita(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Calcula e exibe a correla√ß√£o de todas as vari√°veis num√©ricas com 'variacao_valor_mercado'.\n",
    "    \"\"\"\n",
    "    # Verifica se a coluna 'variacao_valor_mercado' existe\n",
    "    if 'variacao_valor_mercado' not in df.columns:\n",
    "        print(\"Coluna 'variacao_valor_mercado' n√£o encontrada no DataFrame\")\n",
    "        return\n",
    "    \n",
    "    # Seleciona apenas colunas num√©ricas\n",
    "    df_numerico = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Calcula correla√ß√£o com a varia√ß√£o do valor de mercado e ordena\n",
    "    correlacoes = df_numerico.corr()['variacao_valor_mercado'].drop('variacao_valor_mercado').sort_values(key=abs, ascending=False)\n",
    "    \n",
    "    # Heatmap vertical - correla√ß√£o com a varia√ß√£o do valor de mercado\n",
    "    plt.figure(figsize=(8, 15))\n",
    "    \n",
    "    # Cria um DataFrame s√≥ com as correla√ß√µes com a varia√ß√£o do valor de mercado\n",
    "    correlacoes_df = correlacoes.to_frame(name='Correla√ß√£o com a varia√ß√£o do valor de mercado')\n",
    "    \n",
    "    # Heatmap\n",
    "    sns.heatmap(correlacoes_df, \n",
    "                annot=True, \n",
    "                cmap=sns.cubehelix_palette(as_cmap=True),\n",
    "                center=0,\n",
    "                fmt='.3f',\n",
    "                cbar_kws={'label': 'Correla√ß√£o'},\n",
    "                linewidths=0.5)\n",
    "    \n",
    "    plt.title('Correla√ß√£o de todas as vari√°veis com variacao_valor_mercado')\n",
    "    plt.ylabel('Vari√°veis')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNSQpJ34Ayud"
   },
   "source": [
    "Antes de sair selecionando as features com alta correla√ß√£o, vamos definir um threshold (limiar) para considerar se est√° muito correlacionado ou n√£o, sendo um dos nossos crit√©rios de exclus√£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "ccgfCcnOyydp",
    "outputId": "25fbc9af-df3e-4805-8f31-6bda174086ed"
   },
   "outputs": [],
   "source": [
    "calcular_e_exibir_correlacao(df_economatica_correcao_tipagem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SJDmM-Oy4Mw"
   },
   "source": [
    "Visualizando de forma geral, parece estar bem dividido entre fortemente correlacionadas ou n√£o. Vamos considerar uma correla√ß√£o sendo forte como seu valor acima de 0.85.\n",
    "\n",
    "As seguintes vari√°veis est√£o fortemente correlacionadas:\n",
    "- caixa_financeiro <- **0.91** -> dividendos\n",
    "- ano <- **-0.87** -> pandemia\n",
    "- margem_liquida <- **0.96** -> margem_ebit\n",
    "- receita <- **-0.83** -> dividendos\n",
    "- lucro_liquido <- **-0.90** -> dividendos\n",
    "- caixa_financeiro <- **-0.84** -> receita\n",
    "- caixa_financeiro <- **-0.84** -> lucro_liquido\n",
    "- caixa_operacional <- **0.81** -> patrimonio_liquido\n",
    "- caixa_operacional <- **0.90** -> receita\n",
    "- ativo_total <- **0.91** -> receita\n",
    "- lucro_liquido <- **0.81** -> patrimonio_liquido\n",
    "- lucro_liquido <- **0.80** -> receita\n",
    "- receita <- **0.87** -> patrimonio_liquido\n",
    "\n",
    "As seguintes vari√°veis n√£o s√£o consideradas, porque j√° est√£o marcadas para remover:\n",
    "- ativo_circulante\n",
    "- ativo_nao_circulante\n",
    "- passivo_circulante\n",
    "- passivo_nao_circulante"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6EpZeZNvlez"
   },
   "source": [
    "### Colunas derivadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "62kZk2o8y7uJ"
   },
   "source": [
    "As colunas derivadas s√£o:\n",
    "\n",
    "**Endividamento**:\n",
    "- divida_bruta_ativo_* ‚Üí j√° est√° no dataset\n",
    "\n",
    "**Liquidez**:\n",
    "- liquidez_corrente_* = ativo_circulante_* / passivo_circulante_*\n",
    "- liquidez_geral_* = (ativo_circulante_* + ativo_nao_circulante_*) / (passivo_circulante_* + passivo_nao_circulante_*)\n",
    "\n",
    "**Rentabilidade**:\n",
    "- rentabilidade_ativo_* (ROA) = lucro_liquido_* / ((ativo_total_* + ativo_total*_(ano-1)) / 2)\n",
    "- roe_* = lucro_liquido_* / ((patrimonio_liquido_* + patrimonio_liquido*_(ano-1)) / 2)\n",
    "\n",
    "**Margens**:\n",
    "- margem_liquida_* = lucro_liquido_* / receita_*\n",
    "- margem_ebit_* ‚Üí j√° est√° no dataset (mas n√£o pode ser recalculada, pois falta ebit_*)\n",
    "\n",
    "**Lucro por A√ß√£o**:\n",
    "- lpa_* ‚Üí j√° est√° no dataset (n√£o d√° para recalcular, pois falta numero_acoes_*)\n",
    "\n",
    "**Valor de Mercado da Empresa**:\n",
    "- valor_mercado_* ‚Üí j√° est√° no dataset (n√£o d√° para recalcular, pois falta preco_acao_31dez_* e numero_acoes_*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjrYXg4s3U72"
   },
   "source": [
    "#### Liquidez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7veJ1FG3UEn"
   },
   "outputs": [],
   "source": [
    "def analisar_nulos_coluna_derivada(df, coluna_derivada, colunas_calculo):\n",
    "    \"\"\"\n",
    "    Analisa valores nulos em uma coluna derivada e verifica valores n√£o nulos nas colunas de c√°lculo.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): O DataFrame de entrada.\n",
    "        coluna_derivada (str): O nome da coluna derivada (ex: 'liquidez_corrente').\n",
    "        colunas_calculo (list): Uma lista de nomes das colunas usadas para c√°lculo\n",
    "                               (ex: ['ativo_circulante', 'passivo_circulante']).\n",
    "    \"\"\"\n",
    "    print(f\"An√°lise de valores nulos para a coluna {coluna_derivada}:\")\n",
    "\n",
    "    # Verifica se a coluna derivada e as colunas de c√°lculo existem\n",
    "    if coluna_derivada in df.columns and all(c in df.columns for c in colunas_calculo):\n",
    "        # Filtra as linhas onde a coluna derivada √© nula\n",
    "        linhas_com_nulos_derivada = df[df[coluna_derivada].isnull()]\n",
    "\n",
    "        # Constr√≥i a condi√ß√£o para verificar se todas as colunas de c√°lculo n√£o s√£o nulas nessas linhas\n",
    "        if not linhas_com_nulos_derivada.empty:\n",
    "            condicao_nao_nula = linhas_com_nulos_derivada[colunas_calculo[0]].notnull()\n",
    "            for col_calc in colunas_calculo[1:]:\n",
    "                condicao_nao_nula = condicao_nao_nula & linhas_com_nulos_derivada[col_calc].notnull()\n",
    "\n",
    "            # Conta as linhas onde todas as colunas de c√°lculo n√£o s√£o nulas\n",
    "            contagem_nao_nula = linhas_com_nulos_derivada[condicao_nao_nula].shape[0]\n",
    "            \n",
    "            print(f\"Total de linhas com {coluna_derivada} nulo: {len(linhas_com_nulos_derivada)}\")\n",
    "            print(f\"Linhas com valores n√£o nulos nas colunas de c√°lculo: {contagem_nao_nula}\")\n",
    "        else:\n",
    "            print(f\"Nenhuma linha com valor nulo encontrada em {coluna_derivada}\")\n",
    "    else:\n",
    "        colunas_faltando = [col for col in [coluna_derivada] + colunas_calculo if col not in df.columns]\n",
    "        print(f\"Colunas n√£o encontradas no DataFrame: {colunas_faltando}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ONyL454B5P7c",
    "outputId": "c86117b6-0b14-45c0-99cc-c9aeac674b44"
   },
   "outputs": [],
   "source": [
    "analisar_nulos_coluna_derivada(df_economatica_correcao_tipagem, 'liquidez_corrente', ['ativo_circulante', 'passivo_circulante'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Po9QWUh48MSk",
    "outputId": "24f818d2-d391-4ae3-dbe0-48af28b9500c"
   },
   "outputs": [],
   "source": [
    "# Aplica a l√≥gica para liquidez_corrente no dataset com ano como feature\n",
    "# Identifica as linhas onde a coluna derivada √© nula e as colunas de c√°lculo n√£o s√£o nulas\n",
    "condicao = (df_economatica_correcao_tipagem['liquidez_corrente'].isnull()) & \\\n",
    "           (df_economatica_correcao_tipagem['ativo_circulante'].notnull()) & \\\n",
    "           (df_economatica_correcao_tipagem['passivo_circulante'].notnull()) & \\\n",
    "           (df_economatica_correcao_tipagem['passivo_circulante'] != 0) # Evita divis√£o por zero\n",
    "\n",
    "# Calcula o valor derivado\n",
    "valores_calculados = df_economatica_correcao_tipagem.loc[condicao].apply(\n",
    "    lambda row: row['ativo_circulante'] / row['passivo_circulante'], axis=1\n",
    ")\n",
    "\n",
    "# Aplica a fun√ß√£o limpar_e_converter nos valores calculados e preenche os nulos\n",
    "df_economatica_correcao_tipagem.loc[condicao, 'liquidez_corrente'] = valores_calculados.apply(limpar_e_converter)\n",
    "\n",
    "print(f\"Calculados e preenchidos nulos para 'liquidez_corrente' onde 'ativo_circulante' e 'passivo_circulante' n√£o eram nulos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6d80Y26l_ZQN",
    "outputId": "5ac7ae50-212e-4e26-f282-ece6355ea8f8"
   },
   "outputs": [],
   "source": [
    "contar_e_exibir_nulos(df_economatica_correcao_tipagem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECX3ia0T_bBr"
   },
   "source": [
    "Diminuimos em 10, as c√©lulas nulas e nenhuma linha ainda..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "REAEqwyuCFUp",
    "outputId": "9a0ad1c7-ea79-492f-9f38-246525078527"
   },
   "outputs": [],
   "source": [
    "analisar_nulos_coluna_derivada(df_economatica_correcao_tipagem, 'liquidez_geral', ['ativo_circulante', 'ativo_nao_circulante', 'passivo_circulante', 'passivo_nao_circulante'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iTIgrHi8Aw_G",
    "outputId": "36a1421e-4bad-4769-a97e-adc4d739c7d8"
   },
   "outputs": [],
   "source": [
    "condicao = (df_economatica_correcao_tipagem['liquidez_geral'].isnull()) & \\\n",
    "           (df_economatica_correcao_tipagem['ativo_circulante'].notnull()) & \\\n",
    "           (df_economatica_correcao_tipagem['ativo_nao_circulante'].notnull()) & \\\n",
    "           (df_economatica_correcao_tipagem['passivo_circulante'].notnull()) & \\\n",
    "           (df_economatica_correcao_tipagem['passivo_nao_circulante'].notnull())\n",
    "\n",
    "# Calcula o divisor e evita divis√£o por zero\n",
    "divisor = df_economatica_correcao_tipagem.loc[condicao, 'passivo_circulante'] + df_economatica_correcao_tipagem.loc[condicao, 'passivo_nao_circulante']\n",
    "condicao = condicao & (divisor != 0)\n",
    "\n",
    "# Calcula o valor derivado\n",
    "valores_calculados = df_economatica_correcao_tipagem.loc[condicao].apply(\n",
    "    lambda row: (row['ativo_circulante'] + row['ativo_nao_circulante']) / (row['passivo_circulante'] + row['passivo_nao_circulante']), axis=1\n",
    ")\n",
    "\n",
    "# Aplica a fun√ß√£o limpar_e_converter nos valores calculados e preenche os nulos\n",
    "df_economatica_correcao_tipagem.loc[condicao, 'liquidez_geral'] = valores_calculados.apply(limpar_e_converter)\n",
    "\n",
    "print(f\"Calculados e preenchidos nulos para 'liquidez_geral' onde as colunas de c√°lculo n√£o eram nulas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4oWMHGgACQqX",
    "outputId": "c03ac95e-7aa7-4752-ecfd-021f4c08ffc8"
   },
   "outputs": [],
   "source": [
    "contar_e_exibir_nulos(df_economatica_correcao_tipagem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a157Nteq_mAZ"
   },
   "source": [
    "Diminuimos em mais 10, e nenhuma linha recuperada 100% ainda..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoA5Cec6Ci3D"
   },
   "source": [
    "#### Margens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qtP7sg9ADcR4",
    "outputId": "b690a54d-fdac-4af5-ea33-ce5f7cf010b0"
   },
   "outputs": [],
   "source": [
    "analisar_nulos_coluna_derivada(df_economatica_correcao_tipagem, 'margem_liquida', ['lucro_liquido', 'receita'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oo719nDhDMba",
    "outputId": "637d24fd-42b4-4249-83ad-93670414abcf"
   },
   "outputs": [],
   "source": [
    "# Aplica a l√≥gica para margem_liquida no dataset com ano como feature\n",
    "# Identifica as linhas onde a coluna derivada √© nula e as colunas de c√°lculo n√£o s√£o nulas\n",
    "condicao = (df_economatica_correcao_tipagem['margem_liquida'].isnull()) & \\\n",
    "           (df_economatica_correcao_tipagem['lucro_liquido'].notnull()) & \\\n",
    "           (df_economatica_correcao_tipagem['receita'].notnull()) & \\\n",
    "           (df_economatica_correcao_tipagem['receita'] != 0) # Evita divis√£o por zero\n",
    "\n",
    "# Calcula o valor derivado\n",
    "valores_calculados = df_economatica_correcao_tipagem.loc[condicao].apply(\n",
    "    lambda row: row['lucro_liquido'] / row['receita'], axis=1\n",
    ")\n",
    "\n",
    "# Aplica a fun√ß√£o limpar_e_converter nos valores calculados e preenche os nulos\n",
    "df_economatica_correcao_tipagem.loc[condicao, 'margem_liquida'] = valores_calculados.apply(limpar_e_converter)\n",
    "\n",
    "print(f\"Calculados e preenchidos nulos para 'margem_liquida' onde 'lucro_liquido' e 'receita' n√£o eram nulos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F277B9K6D0G8",
    "outputId": "65df09c0-3f58-4fc9-a025-789e5924528f"
   },
   "outputs": [],
   "source": [
    "contar_e_exibir_nulos(df_economatica_correcao_tipagem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ypN7QaOlD1se"
   },
   "source": [
    "Eliminamos incr√≠veis 1 resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wv3lmdUOEAK8"
   },
   "source": [
    "#### Rentabilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZjNdKRx_EXTj",
    "outputId": "3dfac81a-676b-41e2-ed40-a99bfdf99c05"
   },
   "outputs": [],
   "source": [
    "analisar_nulos_coluna_derivada(df_economatica_correcao_tipagem, 'rentabilidade_ativo', ['lucro_liquido', 'ativo_total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9zr4fGxEFOy",
    "outputId": "7a0f11e7-9276-4fda-a5df-53bd537565fb"
   },
   "outputs": [],
   "source": [
    "# Aplica a l√≥gica para rentabilidade_ativo no dataset com ano como feature\n",
    "# Para calcular ROA, precisamos do ativo total do ano anterior, ent√£o criamos um DataFrame auxiliar\n",
    "df_economatica_sorted = df_economatica_correcao_tipagem.sort_values(['codigo', 'ano']).copy()\n",
    "df_economatica_sorted['ativo_total_anterior'] = df_economatica_sorted.groupby('codigo')['ativo_total'].shift(1)\n",
    "\n",
    "# Identifica as linhas onde a coluna derivada √© nula e as colunas de c√°lculo n√£o s√£o nulas\n",
    "condicao = (df_economatica_sorted['rentabilidade_ativo'].isnull()) & \\\n",
    "           (df_economatica_sorted['lucro_liquido'].notnull()) & \\\n",
    "           (df_economatica_sorted['ativo_total'].notnull()) & \\\n",
    "           (df_economatica_sorted['ativo_total_anterior'].notnull()) & \\\n",
    "           (df_economatica_sorted['ano'] > 2020)  # S√≥ calcula a partir de 2021\n",
    "\n",
    "# Calcula o denominador (m√©dia do ativo total atual e anterior) e evita divis√£o por zero\n",
    "denominador = (df_economatica_sorted.loc[condicao, 'ativo_total'] + df_economatica_sorted.loc[condicao, 'ativo_total_anterior']) / 2\n",
    "condicao = condicao & (denominador != 0)\n",
    "\n",
    "# Calcula o valor derivado\n",
    "valores_calculados = df_economatica_sorted.loc[condicao].apply(\n",
    "    lambda row: row['lucro_liquido'] / ((row['ativo_total'] + row['ativo_total_anterior']) / 2), axis=1\n",
    ")\n",
    "\n",
    "# Aplica a fun√ß√£o limpar_e_converter nos valores calculados e preenche os nulos\n",
    "df_economatica_sorted.loc[condicao, 'rentabilidade_ativo'] = valores_calculados.apply(limpar_e_converter)\n",
    "\n",
    "# Atualiza o DataFrame original\n",
    "df_economatica_correcao_tipagem = df_economatica_sorted.drop(columns=['ativo_total_anterior']).copy()\n",
    "\n",
    "print(f\"Calculados e preenchidos nulos para 'rentabilidade_ativo' onde as colunas de c√°lculo n√£o eram nulas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Q6_UqvoEdZV",
    "outputId": "c3d38e74-a809-45e8-f368-0222a19d232f"
   },
   "outputs": [],
   "source": [
    "contar_e_exibir_nulos(df_economatica_correcao_tipagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CvtzRAXCEw2N",
    "outputId": "d2049ae5-7461-4adb-b5cd-53561303a257"
   },
   "outputs": [],
   "source": [
    "analisar_nulos_coluna_derivada(df_economatica_correcao_tipagem, 'roe', ['lucro_liquido', 'patrimonio_liquido'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oFl5dM9oEkP3",
    "outputId": "c08c60f1-1464-432c-a9d3-61668faaf5e6"
   },
   "outputs": [],
   "source": [
    "# Aplica a l√≥gica para roe no dataset com ano como feature\n",
    "# Para calcular ROE, precisamos do patrim√¥nio l√≠quido do ano anterior\n",
    "df_economatica_sorted = df_economatica_correcao_tipagem.sort_values(['codigo', 'ano']).copy()\n",
    "df_economatica_sorted['patrimonio_liquido_anterior'] = df_economatica_sorted.groupby('codigo')['patrimonio_liquido'].shift(1)\n",
    "\n",
    "# Identifica as linhas onde a coluna derivada √© nula e as colunas de c√°lculo n√£o s√£o nulas\n",
    "condicao = (df_economatica_sorted['roe'].isnull()) & \\\n",
    "           (df_economatica_sorted['lucro_liquido'].notnull()) & \\\n",
    "           (df_economatica_sorted['patrimonio_liquido'].notnull()) & \\\n",
    "           (df_economatica_sorted['patrimonio_liquido_anterior'].notnull()) & \\\n",
    "           (df_economatica_sorted['ano'] > 2020)  # S√≥ calcula a partir de 2021\n",
    "\n",
    "# Calcula o denominador (m√©dia do patrim√¥nio l√≠quido atual e anterior) e evita divis√£o por zero\n",
    "denominador = (df_economatica_sorted.loc[condicao, 'patrimonio_liquido'] + df_economatica_sorted.loc[condicao, 'patrimonio_liquido_anterior']) / 2\n",
    "condicao = condicao & (denominador != 0)\n",
    "\n",
    "# Calcula o valor derivado\n",
    "valores_calculados = df_economatica_sorted.loc[condicao].apply(\n",
    "    lambda row: row['lucro_liquido'] / ((row['patrimonio_liquido'] + row['patrimonio_liquido_anterior']) / 2), axis=1\n",
    ")\n",
    "\n",
    "# Aplica a fun√ß√£o limpar_e_converter nos valores calculados e preenche os nulos\n",
    "df_economatica_sorted.loc[condicao, 'roe'] = valores_calculados.apply(limpar_e_converter)\n",
    "\n",
    "# Atualiza o DataFrame original\n",
    "df_economatica_correcao_tipagem = df_economatica_sorted.drop(columns=['patrimonio_liquido_anterior']).copy()\n",
    "\n",
    "print(f\"Calculados e preenchidos nulos para 'roe' onde as colunas de c√°lculo n√£o eram nulas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xJM1YKilE0kF",
    "outputId": "7962a974-d95f-47ed-84a9-330b8694112c"
   },
   "outputs": [],
   "source": [
    "contar_e_exibir_nulos(df_economatica_correcao_tipagem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aqui j√° obtivemos resultados significativos, de 648 linhas com valores nulos, fomos para 509."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remover colunas de ativo/passivo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_economatica_correcao_tipagem.drop(columns=['ativo_circulante', 'ativo_nao_circulante', 'passivo_circulante', 'passivo_nao_circulante'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preenchimento com base em m√©dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identificar_features_com_nulo_unico_ano(df):\n",
    "    \"\"\"\n",
    "    Identifica features onde h√° a√ß√µes que possuem valores nulos em apenas um dos cinco anos (2020-2024).\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame com dados organizados por ano\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dicion√°rio com features como chave e lista de c√≥digos de a√ß√µes que atendem o crit√©rio\n",
    "    \"\"\"\n",
    "    # Colunas que s√£o features num√©ricas (excluindo identificadores e categ√≥ricas)\n",
    "    features_numericas = [col for col in df.columns if col not in ['nome', 'codigo', 'setor', 'ano', 'pandemia']]\n",
    "    \n",
    "    resultado = {}\n",
    "    \n",
    "    for feature in features_numericas:\n",
    "        acoes_com_nulo_unico = []\n",
    "        \n",
    "        # Agrupa por c√≥digo da a√ß√£o\n",
    "        for codigo, grupo in df.groupby('codigo'):\n",
    "            # Verifica se h√° exatamente 5 anos de dados (2020-2024)\n",
    "            if len(grupo) == 5:\n",
    "                # Conta quantos valores nulos existem para essa feature\n",
    "                nulos_count = grupo[feature].isnull().sum()\n",
    "                \n",
    "                # Se h√° exatamente 1 valor nulo (nulo em apenas um ano)\n",
    "                if nulos_count == 1:\n",
    "                    acoes_com_nulo_unico.append(codigo)\n",
    "        \n",
    "        # Se h√° a√ß√µes que atendem o crit√©rio, adiciona ao resultado\n",
    "        if acoes_com_nulo_unico:\n",
    "            resultado[feature] = acoes_com_nulo_unico\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exibir_relatorio_nulos_unicos(df):\n",
    "    \"\"\"\n",
    "    Exibe um relat√≥rio detalhado das features com nulos √∫nicos.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame com dados organizados por ano\n",
    "    \"\"\"\n",
    "    resultado = identificar_features_com_nulo_unico_ano(df)\n",
    "    \n",
    "    if not resultado:\n",
    "        print(\"Nenhuma feature possui a√ß√µes com nulo em apenas um ano.\")\n",
    "        return\n",
    "    \n",
    "    print(\"=== RELAT√ìRIO: Features com nulos em apenas um ano ===\\n\")\n",
    "    \n",
    "    # Lista apenas os nomes das features\n",
    "    features_com_nulo_unico = list(resultado.keys())\n",
    "    print(f\"Total de features identificadas: {len(features_com_nulo_unico)}\")\n",
    "    print(f\"Features: {features_com_nulo_unico}\\n\")\n",
    "    \n",
    "    # Relat√≥rio detalhado por feature\n",
    "    for feature, acoes in resultado.items():\n",
    "        print(f\"Feature: '{feature}'\")\n",
    "        print(f\"  - Quantidade de a√ß√µes afetadas: {len(acoes)}\")\n",
    "        print(f\"  - C√≥digos das a√ß√µes: {acoes[:10]}{'...' if len(acoes) > 10 else ''}\")\n",
    "        \n",
    "        # Mostra em qual ano est√° o nulo para as primeiras 3 a√ß√µes\n",
    "        print(\"  - Detalhes dos primeiros casos:\")\n",
    "        for i, codigo in enumerate(acoes[:3]):\n",
    "            dados_acao = df[df['codigo'] == codigo][['ano', feature]]\n",
    "            ano_nulo = dados_acao[dados_acao[feature].isnull()]['ano'].iloc[0]\n",
    "            print(f\"    * {codigo}: nulo no ano {ano_nulo}\")\n",
    "        print()\n",
    "    \n",
    "    return features_com_nulo_unico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nulo_unico = exibir_relatorio_nulos_unicos(df_economatica_correcao_tipagem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preencher_nulos_com_media_temporal(df):\n",
    "    \"\"\"\n",
    "    Preenche valores nulos com a m√©dia dos outros anos para a√ß√µes que possuem \n",
    "    exatamente 1 valor nulo em uma feature (4 valores v√°lidos e 1 nulo).\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame com dados organizados por ano\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame com os valores nulos preenchidos\n",
    "        dict: Relat√≥rio das altera√ß√µes realizadas\n",
    "    \"\"\"\n",
    "    df_resultado = df.copy()\n",
    "    relatorio = {\n",
    "        'features_processadas': [],\n",
    "        'total_valores_preenchidos': 0,\n",
    "        'detalhes_por_feature': {}\n",
    "    }\n",
    "    \n",
    "    # Identifica features com nulos √∫nicos\n",
    "    features_com_nulo_unico = identificar_features_com_nulo_unico_ano(df)\n",
    "    \n",
    "    if not features_com_nulo_unico:\n",
    "        print(\"Nenhuma feature encontrada com o crit√©rio de 1 nulo em 5 anos.\")\n",
    "        return df_resultado, relatorio\n",
    "    \n",
    "    print(\"=== PREENCHIMENTO COM M√âDIA TEMPORAL ===\\n\")\n",
    "    \n",
    "    for feature, acoes_afetadas in features_com_nulo_unico.items():\n",
    "        valores_preenchidos = 0\n",
    "        detalhes_acoes = []\n",
    "        \n",
    "        print(f\"Processando feature: '{feature}'\")\n",
    "        print(f\"  - A√ß√µes afetadas: {len(acoes_afetadas)}\")\n",
    "        \n",
    "        for codigo in acoes_afetadas:\n",
    "            # Obt√©m os dados da a√ß√£o para todos os anos\n",
    "            dados_acao = df_resultado[df_resultado['codigo'] == codigo].copy()\n",
    "            \n",
    "            if len(dados_acao) == 5:  # Confirma que tem 5 anos de dados\n",
    "                valores_feature = dados_acao[feature]\n",
    "                \n",
    "                # Verifica se h√° exatamente 1 nulo\n",
    "                if valores_feature.isnull().sum() == 1:\n",
    "                    # Calcula a m√©dia dos valores n√£o nulos\n",
    "                    valores_nao_nulos = valores_feature.dropna()\n",
    "                    \n",
    "                    if len(valores_nao_nulos) == 4:  # Confirma que tem 4 valores v√°lidos\n",
    "                        media = valores_nao_nulos.mean()\n",
    "                        \n",
    "                        # Identifica o √≠ndice onde est√° o nulo\n",
    "                        indice_nulo = dados_acao[valores_feature.isnull()].index[0]\n",
    "                        ano_nulo = dados_acao.loc[indice_nulo, 'ano']\n",
    "                        \n",
    "                        # Aplica a fun√ß√£o limpar_e_converter para garantir formata√ß√£o\n",
    "                        media_formatada = limpar_e_converter(media)\n",
    "                        \n",
    "                        # Preenche o valor nulo com a m√©dia\n",
    "                        df_resultado.loc[indice_nulo, feature] = media_formatada\n",
    "                        \n",
    "                        valores_preenchidos += 1\n",
    "                        detalhes_acoes.append({\n",
    "                            'codigo': codigo,\n",
    "                            'ano_nulo': ano_nulo,\n",
    "                            'media_calculada': media_formatada,\n",
    "                            'valores_originais': valores_nao_nulos.tolist()\n",
    "                        })\n",
    "        \n",
    "        # Atualiza o relat√≥rio\n",
    "        relatorio['features_processadas'].append(feature)\n",
    "        relatorio['total_valores_preenchidos'] += valores_preenchidos\n",
    "        relatorio['detalhes_por_feature'][feature] = {\n",
    "            'valores_preenchidos': valores_preenchidos,\n",
    "            'acoes_processadas': detalhes_acoes[:5]  # Mostra apenas os primeiros 5 exemplos\n",
    "        }\n",
    "        \n",
    "        print(f\"  - Valores preenchidos: {valores_preenchidos}\")\n",
    "        \n",
    "        # Mostra alguns exemplos\n",
    "        if detalhes_acoes:\n",
    "            print(\"  - Exemplos:\")\n",
    "            for exemplo in detalhes_acoes[:3]:\n",
    "                print(f\"    * {exemplo['codigo']} (ano {exemplo['ano_nulo']}): \"\n",
    "                      f\"m√©dia = {exemplo['media_calculada']}\")\n",
    "        print()\n",
    "    \n",
    "    print(f\"=== RESUMO ===\")\n",
    "    print(f\"Features processadas: {len(relatorio['features_processadas'])}\")\n",
    "    print(f\"Total de valores preenchidos: {relatorio['total_valores_preenchidos']}\")\n",
    "    \n",
    "    return df_resultado, relatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exibir_relatorio_preenchimento(relatorio):\n",
    "    \"\"\"\n",
    "    Exibe um relat√≥rio detalhado do preenchimento realizado.\n",
    "    \n",
    "    Args:\n",
    "        relatorio (dict): Relat√≥rio retornado pela fun√ß√£o de preenchimento\n",
    "    \"\"\"\n",
    "    print(\"=== RELAT√ìRIO DETALHADO DE PREENCHIMENTO ===\\n\")\n",
    "    \n",
    "    if not relatorio['features_processadas']:\n",
    "        print(\"Nenhum preenchimento foi realizado.\")\n",
    "        return\n",
    "    \n",
    "    for feature in relatorio['features_processadas']:\n",
    "        detalhes = relatorio['detalhes_por_feature'][feature]\n",
    "        print(f\"Feature: '{feature}'\")\n",
    "        print(f\"  - Valores preenchidos: {detalhes['valores_preenchidos']}\")\n",
    "        \n",
    "        if detalhes['acoes_processadas']:\n",
    "            print(\"  - Detalhes das a√ß√µes processadas:\")\n",
    "            for acao in detalhes['acoes_processadas']:\n",
    "                print(f\"    * {acao['codigo']} (ano {acao['ano_nulo']}): \"\n",
    "                      f\"m√©dia = {acao['media_calculada']}\")\n",
    "                print(f\"      Valores originais: {acao['valores_originais']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Antes do preenchimento:\")\n",
    "contar_e_exibir_nulos(df_economatica_correcao_tipagem)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "df_economatica_preenchido, relatorio_preenchimento = preencher_nulos_com_media_temporal(df_economatica_correcao_tipagem)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "print(\"Ap√≥s o preenchimento:\")\n",
    "contar_e_exibir_nulos(df_economatica_preenchido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atribui√ß√£o de valor para a vari√°vel target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiro, garantir que a coluna valor_mercado esteja corretamente limpa e convertida\n",
    "df_economatica_preenchido['valor_mercado'] = df_economatica_preenchido['valor_mercado'].apply(limpar_e_converter)\n",
    "\n",
    "# Ordenar por c√≥digo e ano para c√°lculo correto\n",
    "df_economatica_preenchido = df_economatica_preenchido.sort_values(by=['codigo', 'ano'])\n",
    "\n",
    "# Calcular valor de mercado do ano anterior\n",
    "df_economatica_preenchido['valor_mercado_anterior'] = df_economatica_preenchido.groupby('codigo')['valor_mercado'].shift(1)\n",
    "\n",
    "# Calcular a varia√ß√£o do valor de mercado\n",
    "df_economatica_preenchido['variacao_valor_mercado_raw'] = (df_economatica_preenchido['valor_mercado'] / df_economatica_preenchido['valor_mercado_anterior']) - 1\n",
    "\n",
    "# Aplicar a l√≥gica da vari√°vel target:\n",
    "# - Se valor_mercado for NaN, target = NaN\n",
    "# - Se n√£o h√° ano anterior (primeiro ano), target = NaN\n",
    "# - Caso contr√°rio, retorna a varia√ß√£o arredondada em 4 casas decimais\n",
    "def calcular_target(row):\n",
    "    \"\"\"\n",
    "    Calcula a vari√°vel target baseada nas regras definidas:\n",
    "    - Se valor_mercado √© NaN, retorna NaN\n",
    "    - Se n√£o h√° ano anterior, retorna NaN\n",
    "    - Caso contr√°rio, retorna a varia√ß√£o arredondada em 4 casas decimais\n",
    "    \"\"\"\n",
    "    if pd.isna(row['valor_mercado']) or pd.isna(row['valor_mercado_anterior']):\n",
    "        return np.nan\n",
    "    \n",
    "    variacao = row['variacao_valor_mercado_raw']\n",
    "    \n",
    "    if pd.isna(variacao):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return round(variacao, 4)\n",
    "\n",
    "# Aplicar a fun√ß√£o para calcular a target\n",
    "df_economatica_preenchido['variacao_valor_mercado'] = df_economatica_preenchido.apply(calcular_target, axis=1)\n",
    "\n",
    "# Remover colunas auxiliares\n",
    "df_economatica_preenchido = df_economatica_preenchido.drop(columns=['valor_mercado', 'valor_mercado_anterior', 'variacao_valor_mercado_raw'])\n",
    "\n",
    "print(\"--- Vari√°vel Target criada ---\")\n",
    "print(\"Target: Varia√ß√£o percentual do valor de mercado arredondada em 4 casas decimais\")\n",
    "print(\"Valores positivos = valoriza√ß√£o, valores negativos = desvaloriza√ß√£o\")\n",
    "print(\"Valores NaN = primeiro ano de cada empresa (n√£o h√° ano anterior para compara√ß√£o)\\n\")\n",
    "\n",
    "print(\"Estat√≠sticas da vari√°vel target:\")\n",
    "print(df_economatica_preenchido['variacao_valor_mercado'].describe())\n",
    "\n",
    "print(f\"\\nValores nulos na target: {df_economatica_preenchido['variacao_valor_mercado'].isnull().sum()}\")\n",
    "print(f\"Tipo da vari√°vel target (valores n√£o nulos): {df_economatica_preenchido['variacao_valor_mercado'].dropna().dtype}\")\n",
    "\n",
    "# Calcula estat√≠sticas apenas dos valores n√£o nulos\n",
    "valores_nao_nulos = df_economatica_preenchido['variacao_valor_mercado'].dropna()\n",
    "if len(valores_nao_nulos) > 0:\n",
    "    valorizacoes = (valores_nao_nulos > 0).sum()\n",
    "    total_validos = len(valores_nao_nulos)\n",
    "    print(f\"Percentual de valoriza√ß√µes (varia√ß√£o > 0): {(valorizacoes / total_validos) * 100:.1f}%\")\n",
    "    print(f\"Varia√ß√£o m√©dia: {valores_nao_nulos.mean():.4f}\")\n",
    "    print(f\"Varia√ß√£o m√≠nima: {valores_nao_nulos.min():.4f}\")\n",
    "    print(f\"Varia√ß√£o m√°xima: {valores_nao_nulos.max():.4f}\")\n",
    "\n",
    "print(\"\\nVisualiza√ß√£o do DataFrame final:\")\n",
    "print(df_economatica_preenchido[['nome', 'codigo', 'ano', 'variacao_valor_mercado']].head(20))\n",
    "\n",
    "print(f\"\\nDistribui√ß√£o por ano:\")\n",
    "print(df_economatica_preenchido.groupby('ano')['variacao_valor_mercado'].agg(['count', lambda x: x.isnull().sum()]).rename(columns={'<lambda>': 'nulos'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìÖ DISTRIBUI√á√ÉO POR ANO:\")\n",
    "distribuicao_ano = df_economatica_preenchido['ano'].value_counts().sort_index()\n",
    "for ano, count in distribuicao_ano.items():\n",
    "  print(f\"  - {ano}: {count:,} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remo√ß√£o atrav√©s da correla√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calcular_correlacao_com_receita(df_economatica_preenchido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An√°lise de Correla√ß√£o e Remo√ß√£o de Vari√°veis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com base nos dados de correla√ß√£o fornecidos, a principal estrat√©gia √© identificar e remover as vari√°veis que mostram uma forte multicolinearidade (alta correla√ß√£o entre si) e que, ao mesmo tempo, t√™m a menor correla√ß√£o com a vari√°vel-alvo, que √© **variacao_valor_mercado**.\n",
    "\n",
    "---\n",
    "\n",
    "#### Frequ√™ncia de Ocorr√™ncia em Pares Correlacionados\n",
    "\n",
    "Primeiro, foi analisada a frequ√™ncia com que cada vari√°vel aparece em um par de alta correla√ß√£o:\n",
    "\n",
    "- **receita:** 5 ocorr√™ncias  \n",
    "- **lucro_liquido:** 4 ocorr√™ncias  \n",
    "- **caixa_financeiro:** 3 ocorr√™ncias  \n",
    "- **dividendos:** 3 ocorr√™ncias  \n",
    "- **patrimonio_liquido:** 3 ocorr√™ncias  \n",
    "- **caixa_operacional:** 2 ocorr√™ncias  \n",
    "- **margem_liquida:** 1 ocorr√™ncia  \n",
    "- **margem_ebit:** 1 ocorr√™ncia  \n",
    "- **pandemia:** 1 ocorr√™ncia  \n",
    "- **ano:** 1 ocorr√™ncia  \n",
    "- **ativo_total:** 1 ocorr√™ncia  \n",
    "\n",
    "---\n",
    "\n",
    "#### Decis√£o de Remo√ß√£o por Par\n",
    "\n",
    "Em cada par de vari√°veis com forte correla√ß√£o, a decis√£o de qual remover foi baseada na que possui a correla√ß√£o mais pr√≥xima de zero com a vari√°vel **variacao_valor_mercado**:\n",
    "\n",
    "- **caixa_financeiro vs. dividendos:**  \n",
    "  - caixa_financeiro (-0.007) < dividendos (0.008) ‚Üí **Remover caixa_financeiro**  \n",
    "\n",
    "- **ano vs. pandemia:**  \n",
    "  - ano (-0.016) > pandemia (-0.133) ‚Üí **Remover ano**  \n",
    "\n",
    "- **margem_liquida vs. margem_ebit:**  \n",
    "  - margem_liquida (-0.012) > margem_ebit (-0.014) ‚Üí **Remover margem_liquida**  \n",
    "\n",
    "- **receita vs. dividendos:**  \n",
    "  - receita (-0.008) < dividendos (0.008) ‚Üí **Remover receita**  \n",
    "\n",
    "- **lucro_liquido vs. dividendos:**  \n",
    "  - lucro_liquido (0.020) > dividendos (0.008) ‚Üí **Remover dividendos**  \n",
    "\n",
    "- **caixa_financeiro vs. receita:**  \n",
    "  - caixa_financeiro (-0.007) > receita (-0.008) ‚Üí **Remover caixa_financeiro**  \n",
    "\n",
    "- **caixa_financeiro vs. lucro_liquido:**  \n",
    "  - caixa_financeiro (-0.007) < lucro_liquido (0.020) ‚Üí **Remover caixa_financeiro**  \n",
    "\n",
    "- **caixa_operacional vs. patrimonio_liquido:**  \n",
    "  - patrimonio_liquido (0.006) < caixa_operacional (0.023) ‚Üí **Remover patrimonio_liquido**  \n",
    "\n",
    "- **caixa_operacional vs. receita:**  \n",
    "  - receita (-0.008) < caixa_operacional (0.023) ‚Üí **Remover receita**  \n",
    "\n",
    "- **ativo_total vs. receita:**  \n",
    "  - ativo_total (0.007) < receita (-0.008) ‚Üí **Remover ativo_total**  \n",
    "\n",
    "- **lucro_liquido vs. patrimonio_liquido:**  \n",
    "  - patrimonio_liquido (0.006) < lucro_liquido (0.020) ‚Üí **Remover patrimonio_liquido**  \n",
    "\n",
    "- **lucro_liquido vs. receita:**  \n",
    "  - receita (-0.008) < lucro_liquido (0.020) ‚Üí **Remover receita**  \n",
    "\n",
    "- **receita vs. patrimonio_liquido:**  \n",
    "  - patrimonio_liquido (0.006) < receita (-0.008) ‚Üí **Remover patrimonio_liquido**  \n",
    "\n",
    "---\n",
    "\n",
    "#### Lista Final de Vari√°veis para Remo√ß√£o\n",
    "\n",
    "Com base na an√°lise, estas s√£o as vari√°veis que, consistentemente, devem ser removidas para otimizar o modelo:\n",
    "\n",
    "- **receita** (5 indica√ß√µes de remo√ß√£o)  \n",
    "- **caixa_financeiro** (3 indica√ß√µes de remo√ß√£o)  \n",
    "- **patrimonio_liquido** (3 indica√ß√µes de remo√ß√£o)  \n",
    "- **dividendos** (2 indica√ß√µes de remo√ß√£o)  \n",
    "- **ano** (1 indica√ß√£o de remo√ß√£o)  \n",
    "- **margem_liquida** (1 indica√ß√£o de remo√ß√£o)  \n",
    "- **ativo_total** (1 indica√ß√£o de remo√ß√£o)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_economatica_preenchido.drop(columns=['receita', 'caixa_financeiro', 'patrimonio_liquido', 'dividendos', 'margem_liquida', 'ativo_total'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìÖ DISTRIBUI√á√ÉO POR ANO:\")\n",
    "distribuicao_ano = df_economatica_preenchido['ano'].value_counts().sort_index()\n",
    "for ano, count in distribuicao_ano.items():\n",
    "    print(f\"  - {ano}: {count:,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contar_e_exibir_nulos(df_economatica_preenchido)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remover o ano de 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover todos os registros do ano 2020\n",
    "df_economatica_preenchido = df_economatica_preenchido[df_economatica_preenchido['ano'] != 2020].copy()\n",
    "\n",
    "print(\"=== REMO√á√ÉO DOS REGISTROS DE 2020 ===\")\n",
    "print(f\"Registros restantes ap√≥s remo√ß√£o do ano 2020:\")\n",
    "print(f\"Shape do DataFrame: {df_economatica_preenchido.shape}\")\n",
    "\n",
    "print(\"\\nDistribui√ß√£o por ano ap√≥s remo√ß√£o:\")\n",
    "print(df_economatica_preenchido['ano'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nDistribui√ß√£o da pandemia ap√≥s remo√ß√£o:\")\n",
    "print(df_economatica_preenchido['pandemia'].value_counts())\n",
    "\n",
    "print(\"\\nVerifica√ß√£o - anos √∫nicos restantes:\")\n",
    "print(sorted(df_economatica_preenchido['ano'].unique()))\n",
    "\n",
    "print(\"\\nDistribui√ß√£o da vari√°vel target ap√≥s remo√ß√£o de 2020:\")\n",
    "print(df_economatica_preenchido['variacao_valor_mercado'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nValores nulos na target ap√≥s remo√ß√£o: {df_economatica_preenchido['variacao_valor_mercado'].isnull().sum()}\")\n",
    "\n",
    "# Verifica quantos c√≥digos √∫nicos restaram\n",
    "print(f\"\\nC√≥digos √∫nicos restantes: {df_economatica_preenchido['codigo'].nunique()}\")\n",
    "\n",
    "# Verifica se algum c√≥digo ficou com menos de 4 anos de dados\n",
    "contagem_por_codigo = df_economatica_preenchido['codigo'].value_counts()\n",
    "codigos_incompletos = contagem_por_codigo[contagem_por_codigo < 4]\n",
    "print(f\"C√≥digos com menos de 4 anos de dados: {len(codigos_incompletos)}\")\n",
    "\n",
    "if len(codigos_incompletos) > 0:\n",
    "    print(\"C√≥digos incompletos:\", codigos_incompletos.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remo√ß√£o dos nulos restantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identificar_codigos_com_nulos(df):\n",
    "    \"\"\"\n",
    "    Identifica as linhas que possuem valores nulos e retorna a lista de c√≥digos √∫nicos.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame com dados organizados por ano\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dicion√°rio com informa√ß√µes detalhadas sobre c√≥digos com nulos\n",
    "    \"\"\"\n",
    "    # Identifica linhas que possuem pelo menos um valor nulo\n",
    "    linhas_com_nulos = df.isnull().any(axis=1)\n",
    "    \n",
    "    # Filtra o DataFrame para apenas linhas com nulos\n",
    "    df_com_nulos = df[linhas_com_nulos].copy()\n",
    "    \n",
    "    # Lista de c√≥digos √∫nicos que possuem linhas com nulos\n",
    "    codigos_com_nulos = df_com_nulos['codigo'].unique().tolist()\n",
    "    \n",
    "    # Contagem de linhas com nulos por c√≥digo\n",
    "    contagem_por_codigo = df_com_nulos['codigo'].value_counts().to_dict()\n",
    "    \n",
    "    # An√°lise detalhada por c√≥digo\n",
    "    detalhes_por_codigo = {}\n",
    "    for codigo in codigos_com_nulos:\n",
    "        dados_codigo = df[df['codigo'] == codigo]\n",
    "        linhas_nulas_codigo = dados_codigo.isnull().any(axis=1).sum()\n",
    "        total_linhas_codigo = len(dados_codigo)\n",
    "        anos_com_nulos = dados_codigo[dados_codigo.isnull().any(axis=1)]['ano'].tolist()\n",
    "        \n",
    "        detalhes_por_codigo[codigo] = {\n",
    "            'total_linhas': total_linhas_codigo,\n",
    "            'linhas_com_nulos': linhas_nulas_codigo,\n",
    "            'anos_com_nulos': anos_com_nulos,\n",
    "            'percentual_nulos': round((linhas_nulas_codigo / total_linhas_codigo) * 100, 2)\n",
    "        }\n",
    "    \n",
    "    # Resumo geral\n",
    "    resumo = {\n",
    "        'total_linhas_dataset': len(df),\n",
    "        'total_linhas_com_nulos': len(df_com_nulos),\n",
    "        'total_codigos_unicos': df['codigo'].nunique(),\n",
    "        'total_codigos_com_nulos': len(codigos_com_nulos),\n",
    "        'percentual_codigos_afetados': round((len(codigos_com_nulos) / df['codigo'].nunique()) * 100, 2)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'codigos_com_nulos': codigos_com_nulos,\n",
    "        'contagem_por_codigo': contagem_por_codigo,\n",
    "        'detalhes_por_codigo': detalhes_por_codigo,\n",
    "        'resumo': resumo\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exibir_relatorio_codigos_nulos(df):\n",
    "    \"\"\"\n",
    "    Exibe um relat√≥rio completo dos c√≥digos que possuem linhas com valores nulos.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame com dados organizados por ano\n",
    "    \"\"\"\n",
    "    resultado = identificar_codigos_com_nulos(df)\n",
    "    \n",
    "    print(\"=== RELAT√ìRIO: C√≥digos com Valores Nulos ===\\n\")\n",
    "    \n",
    "    # Resumo geral\n",
    "    resumo = resultado['resumo']\n",
    "    print(f\"üìä RESUMO GERAL:\")\n",
    "    print(f\"  - Total de linhas no dataset: {resumo['total_linhas_dataset']:,}\")\n",
    "    print(f\"  - Linhas com pelo menos um nulo: {resumo['total_linhas_com_nulos']:,}\")\n",
    "    print(f\"  - Total de c√≥digos √∫nicos: {resumo['total_codigos_unicos']:,}\")\n",
    "    print(f\"  - C√≥digos com nulos: {resumo['total_codigos_com_nulos']:,}\")\n",
    "    print(f\"  - Percentual de c√≥digos afetados: {resumo['percentual_codigos_afetados']}%\")\n",
    "    print()\n",
    "    \n",
    "    # Lista dos c√≥digos\n",
    "    codigos = resultado['codigos_com_nulos']\n",
    "    print(f\"üìã LISTA DE C√ìDIGOS COM NULOS ({len(codigos)} c√≥digos):\")\n",
    "    \n",
    "    # Mostra em grupos de 10 para melhor visualiza√ß√£o\n",
    "    for i in range(0, len(codigos), 10):\n",
    "        grupo = codigos[i:i+10]\n",
    "        print(f\"  {grupo}\")\n",
    "    print()\n",
    "    \n",
    "    # Top 10 c√≥digos com mais linhas nulas\n",
    "    contagem = resultado['contagem_por_codigo']\n",
    "    top_10_nulos = sorted(contagem.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    print(f\"üîù TOP 10 C√ìDIGOS COM MAIS LINHAS NULAS:\")\n",
    "    for codigo, qtd_linhas in top_10_nulos:\n",
    "        detalhes = resultado['detalhes_por_codigo'][codigo]\n",
    "        print(f\"  - {codigo}: {qtd_linhas}/{detalhes['total_linhas']} linhas \"\n",
    "              f\"({detalhes['percentual_nulos']}%) - Anos: {detalhes['anos_com_nulos']}\")\n",
    "    print()\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obter_lista_codigos_com_nulos(df):\n",
    "    \"\"\"\n",
    "    Retorna apenas a lista simples de c√≥digos √∫nicos que possuem valores nulos.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame com dados organizados por ano\n",
    "        \n",
    "    Returns:\n",
    "        list: Lista de c√≥digos √∫nicos com valores nulos\n",
    "    \"\"\"\n",
    "    resultado = identificar_codigos_com_nulos(df)\n",
    "    return resultado['codigos_com_nulos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analisar_padroes_nulos_por_codigo(df, codigo_especifico=None):\n",
    "    \"\"\"\n",
    "    Analisa padr√µes detalhados de nulos para um c√≥digo espec√≠fico ou todos os c√≥digos.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame com dados organizados por ano\n",
    "        codigo_especifico (str, optional): C√≥digo espec√≠fico para an√°lise detalhada\n",
    "        \n",
    "    Returns:\n",
    "        dict: An√°lise detalhada dos padr√µes de nulos\n",
    "    \"\"\"\n",
    "    if codigo_especifico:\n",
    "        # An√°lise para um c√≥digo espec√≠fico\n",
    "        dados_codigo = df[df['codigo'] == codigo_especifico].copy()\n",
    "        if dados_codigo.empty:\n",
    "            return f\"C√≥digo '{codigo_especifico}' n√£o encontrado no dataset.\"\n",
    "        \n",
    "        print(f\"=== AN√ÅLISE DETALHADA: {codigo_especifico} ===\\n\")\n",
    "        \n",
    "        # Para cada ano, mostra quais colunas t√™m nulos\n",
    "        for _, linha in dados_codigo.iterrows():\n",
    "            ano = linha['ano']\n",
    "            colunas_nulas = linha.isnull()\n",
    "            colunas_com_nulos = colunas_nulas[colunas_nulas].index.tolist()\n",
    "            \n",
    "            print(f\"üìÖ Ano {ano}:\")\n",
    "            if colunas_com_nulos:\n",
    "                # Remove colunas de identifica√ß√£o da lista\n",
    "                colunas_features_nulas = [col for col in colunas_com_nulos \n",
    "                                        if col not in ['nome', 'codigo', 'setor']]\n",
    "                print(f\"  - Colunas com nulos: {colunas_features_nulas}\")\n",
    "                print(f\"  - Total de nulos: {len(colunas_features_nulas)}\")\n",
    "            else:\n",
    "                print(\"  - Sem valores nulos\")\n",
    "            print()\n",
    "    else:\n",
    "        # An√°lise geral de padr√µes\n",
    "        resultado = identificar_codigos_com_nulos(df)\n",
    "        return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== IDENTIFICA√á√ÉO DE C√ìDIGOS COM VALORES NULOS ===\\n\")\n",
    "relatorio_nulos = exibir_relatorio_codigos_nulos(df_economatica_preenchido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remover_acoes_com_nulos(df):\n",
    "    \"\"\"\n",
    "    Remove TODAS as a√ß√µes que possuem pelo menos 1 valor nulo em qualquer ano/feature.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame com dados organizados por ano\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame apenas com a√ß√µes que n√£o possuem nenhum valor nulo\n",
    "        dict: Relat√≥rio das remo√ß√µes realizadas\n",
    "    \"\"\"\n",
    "    print(\"=== REMO√á√ÉO DE A√á√ïES COM VALORES NULOS ===\\n\")\n",
    "    \n",
    "    # Estado inicial\n",
    "    total_linhas_inicial = len(df)\n",
    "    total_codigos_inicial = df['codigo'].nunique()\n",
    "    \n",
    "    print(f\"üìä ESTADO INICIAL:\")\n",
    "    print(f\"  - Total de linhas: {total_linhas_inicial:,}\")\n",
    "    print(f\"  - Total de c√≥digos √∫nicos: {total_codigos_inicial:,}\")\n",
    "    print()\n",
    "    \n",
    "    # Identifica c√≥digos com nulos\n",
    "    resultado_nulos = identificar_codigos_com_nulos(df)\n",
    "    codigos_com_nulos = resultado_nulos['codigos_com_nulos']\n",
    "    \n",
    "    print(f\"üîç AN√ÅLISE DE NULOS:\")\n",
    "    print(f\"  - C√≥digos com pelo menos 1 nulo: {len(codigos_com_nulos):,}\")\n",
    "    print(f\"  - C√≥digos sem nulos: {total_codigos_inicial - len(codigos_com_nulos):,}\")\n",
    "    print()\n",
    "    \n",
    "    # Remove todas as linhas dos c√≥digos que possuem nulos\n",
    "    df_limpo = df[~df['codigo'].isin(codigos_com_nulos)].copy()\n",
    "    \n",
    "    # Estado final\n",
    "    total_linhas_final = len(df_limpo)\n",
    "    total_codigos_final = df_limpo['codigo'].nunique()\n",
    "    \n",
    "    linhas_removidas = total_linhas_inicial - total_linhas_final\n",
    "    codigos_removidos = total_codigos_inicial - total_codigos_final\n",
    "    \n",
    "    print(f\"üóëÔ∏è REMO√á√ïES REALIZADAS:\")\n",
    "    print(f\"  - Linhas removidas: {linhas_removidas:,}\")\n",
    "    print(f\"  - C√≥digos removidos: {codigos_removidos:,}\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"‚úÖ ESTADO FINAL:\")\n",
    "    print(f\"  - Total de linhas: {total_linhas_final:,}\")\n",
    "    print(f\"  - Total de c√≥digos √∫nicos: {total_codigos_final:,}\")\n",
    "    print(f\"  - Percentual de linhas mantidas: {(total_linhas_final/total_linhas_inicial)*100:.1f}%\")\n",
    "    print(f\"  - Percentual de c√≥digos mantidos: {(total_codigos_final/total_codigos_inicial)*100:.1f}%\")\n",
    "    print()\n",
    "    \n",
    "    # Verifica se ainda h√° nulos (deve ser 0)\n",
    "    nulos_restantes = df_limpo.isnull().sum().sum()\n",
    "    print(f\"üîç VERIFICA√á√ÉO FINAL:\")\n",
    "    print(f\"  - Valores nulos restantes: {nulos_restantes}\")\n",
    "    \n",
    "    if nulos_restantes == 0:\n",
    "        print(\"  ‚úÖ Sucesso! Nenhum valor nulo encontrado no dataset final.\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è Aten√ß√£o! Ainda h√° valores nulos no dataset.\")\n",
    "    \n",
    "    # Relat√≥rio detalhado\n",
    "    relatorio = {\n",
    "        'estado_inicial': {\n",
    "            'total_linhas': total_linhas_inicial,\n",
    "            'total_codigos': total_codigos_inicial\n",
    "        },\n",
    "        'estado_final': {\n",
    "            'total_linhas': total_linhas_final,\n",
    "            'total_codigos': total_codigos_final\n",
    "        },\n",
    "        'remocoes': {\n",
    "            'linhas_removidas': linhas_removidas,\n",
    "            'codigos_removidos': codigos_removidos,\n",
    "            'codigos_com_nulos': codigos_com_nulos\n",
    "        },\n",
    "        'percentuais': {\n",
    "            'linhas_mantidas': round((total_linhas_final/total_linhas_inicial)*100, 2),\n",
    "            'codigos_mantidos': round((total_codigos_final/total_codigos_inicial)*100, 2)\n",
    "        },\n",
    "        'verificacao': {\n",
    "            'nulos_restantes': nulos_restantes,\n",
    "            'dataset_limpo': nulos_restantes == 0\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return df_limpo, relatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exibir_estatisticas_pos_remocao(df_limpo, relatorio):\n",
    "    \"\"\"\n",
    "    Exibe estat√≠sticas detalhadas ap√≥s a remo√ß√£o das a√ß√µes com nulos.\n",
    "    \n",
    "    Args:\n",
    "        df_limpo (pd.DataFrame): DataFrame ap√≥s remo√ß√£o\n",
    "        relatorio (dict): Relat√≥rio da remo√ß√£o\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìà ESTAT√çSTICAS DETALHADAS P√ìS-REMO√á√ÉO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Distribui√ß√£o por ano\n",
    "    print(\"\\nüìÖ DISTRIBUI√á√ÉO POR ANO:\")\n",
    "    distribuicao_ano = df_limpo['ano'].value_counts().sort_index()\n",
    "    for ano, count in distribuicao_ano.items():\n",
    "        print(f\"  - {ano}: {count:,} registros\")\n",
    "    \n",
    "    # Verifica se todos os c√≥digos t√™m 4 anos de dados\n",
    "    contagem_por_codigo = df_limpo['codigo'].value_counts()\n",
    "    codigos_completos = (contagem_por_codigo == 4).sum()\n",
    "    codigos_incompletos = (contagem_por_codigo != 4).sum()\n",
    "    \n",
    "    print(f\"\\nüîç INTEGRIDADE DOS DADOS:\")\n",
    "    print(f\"  - C√≥digos com 4 anos completos: {codigos_completos:,}\")\n",
    "    print(f\"  - C√≥digos com dados incompletos: {codigos_incompletos:,}\")\n",
    "    \n",
    "    if codigos_incompletos > 0:\n",
    "        print(f\"  ‚ö†Ô∏è Aten√ß√£o: {codigos_incompletos} c√≥digos n√£o t√™m 4 anos de dados!\")\n",
    "        incompletos = contagem_por_codigo[contagem_por_codigo != 4]\n",
    "        print(f\"  - Detalhes: {incompletos.to_dict()}\")\n",
    "    \n",
    "    # Estat√≠sticas de colunas\n",
    "    print(f\"\\nüìä ESTAT√çSTICAS DO DATASET:\")\n",
    "    print(f\"  - N√∫mero de colunas: {df_limpo.shape[1]}\")\n",
    "    print(f\"  - Colunas num√©ricas: {len(df_limpo.select_dtypes(include=[np.number]).columns)}\")\n",
    "    print(f\"  - Densidade dos dados: 100% (sem nulos)\")\n",
    "    \n",
    "    # Lista algumas a√ß√µes que permaneceram\n",
    "    print(f\"\\n‚úÖ EXEMPLOS DE A√á√ïES MANTIDAS:\")\n",
    "    acoes_exemplo = df_limpo['codigo'].unique()[:10]\n",
    "    print(f\"  - Primeiras 10: {list(acoes_exemplo)}\")\n",
    "    \n",
    "    return distribuicao_ano, contagem_por_codigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ANTES DA REMO√á√ÉO:\")\n",
    "contar_e_exibir_nulos(df_economatica_preenchido)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "df_economatica_sem_nulos, relatorio_remocao = remover_acoes_com_nulos(df_economatica_preenchido)\n",
    "\n",
    "# Exibe estat√≠sticas detalhadas\n",
    "estatisticas = exibir_estatisticas_pos_remocao(df_economatica_sem_nulos, relatorio_remocao)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ VERIFICA√á√ÉO FINAL DE NULOS:\")\n",
    "contar_e_exibir_nulos(df_economatica_sem_nulos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìÖ DISTRIBUI√á√ÉO POR ANO:\")\n",
    "distribuicao_ano = df_economatica_sem_nulos['ano'].value_counts().sort_index()\n",
    "for ano, count in distribuicao_ano.items():\n",
    "  print(f\"  - {ano}: {count:,} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerar CSV do dataframe limpo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar CSV do dataframe limpo\n",
    "print(\"=== EXPORTA√á√ÉO DO DATASET LIMPO ===\\n\")\n",
    "\n",
    "# Define o caminho baseado no ambiente\n",
    "if is_colab:\n",
    "    caminho_csv_limpo = '/content/df_economatica_limpo.csv'\n",
    "else:\n",
    "    os.makedirs('../datasets', exist_ok=True)\n",
    "    caminho_csv_limpo = '../datasets/df_economatica_limpo.csv'\n",
    "\n",
    "# Exporta o CSV\n",
    "df_economatica_sem_nulos.to_csv(caminho_csv_limpo, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"‚úÖ Dataset limpo exportado para: {caminho_csv_limpo}\")\n",
    "print(f\"üìä Shape do dataset: {df_economatica_sem_nulos.shape}\")\n",
    "print(f\"üìÖ Per√≠odo: {df_economatica_sem_nulos['ano'].min()} - {df_economatica_sem_nulos['ano'].max()}\")\n",
    "print(f\"üè¢ Empresas √∫nicas: {df_economatica_sem_nulos['codigo'].nunique()}\")\n",
    "print(f\"üìà Colunas: {df_economatica_sem_nulos.shape[1]}\")\n",
    "\n",
    "# Resumo das colunas\n",
    "print(f\"\\nüìã RESUMO DAS COLUNAS:\")\n",
    "colunas_por_tipo = {\n",
    "    'Identifica√ß√£o': ['nome', 'codigo'],\n",
    "    'Categ√≥ricas': ['setor'],\n",
    "    'Temporais': ['ano', 'pandemia'],\n",
    "    'Target': ['variacao_valor_mercado'],\n",
    "    'Features Financeiras': [col for col in df_economatica_sem_nulos.columns \n",
    "                           if col not in ['nome', 'codigo', 'setor', 'ano', 'pandemia', 'variacao_valor_mercado']]\n",
    "}\n",
    "\n",
    "for tipo, colunas in colunas_por_tipo.items():\n",
    "    colunas_existentes = [col for col in colunas if col in df_economatica_sem_nulos.columns]\n",
    "    print(f\"  - {tipo}: {len(colunas_existentes)} colunas\")\n",
    "\n",
    "print(f\"\\nüîç VERIFICA√á√ïES FINAIS:\")\n",
    "print(f\"  - Valores nulos: {df_economatica_sem_nulos.isnull().sum().sum()}\")\n",
    "print(f\"  - Linhas duplicadas: {df_economatica_sem_nulos.duplicated().sum()}\")\n",
    "print(f\"  - Registros por empresa: {df_economatica_sem_nulos['codigo'].value_counts().unique()}\")\n",
    "\n",
    "# Mostra amostra dos dados\n",
    "print(f\"\\nüëÄ AMOSTRA DOS DADOS:\")\n",
    "print(df_economatica_sem_nulos.head())\n",
    "\n",
    "print(f\"\\nüìÅ Arquivo salvo com sucesso!\")\n",
    "print(f\"   Caminho: {caminho_csv_limpo}\")\n",
    "print(f\"   Tamanho estimado: ~{df_economatica_sem_nulos.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An√°lise explorat√≥ria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_economatica_eda = df_economatica_sem_nulos.copy()\n",
    "df_economatica_eda.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estat√≠stica descritiva"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos tentar identificar grande esparcidade nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_economatica_eda.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterando sobre todas as colunas num√©ricas e mostrando min, max e m√©dia\n",
    "print(\"=== ESTAT√çSTICAS DAS COLUNAS NUM√âRICAS ===\\n\")\n",
    "\n",
    "colunas_numericas = [col for col in df_economatica_eda.columns if col not in ['nome', 'codigo', 'setor', 'pandemia', 'ano']]\n",
    "\n",
    "print(f\"Total de colunas num√©ricas encontradas: {len(colunas_numericas)}\\n\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, coluna in enumerate(colunas_numericas, 1):\n",
    "    print(f\"{i:2d}. Coluna: {coluna}\")\n",
    "    print(f\"    M√≠nimo: {df_economatica_eda[coluna].min():.4f}\")\n",
    "    print(f\"    M√°ximo: {df_economatica_eda[coluna].max():.4f}\")\n",
    "    print(f\"    M√©dia:  {df_economatica_eda[coluna].mean():.4f}\")\n",
    "    print(f\"    Amplitude total: {(df_economatica_eda[coluna].max() - df_economatica_eda[coluna].min()):.4f}\")\n",
    "    print(f\"    Vari√¢ncia: {df_economatica_eda[coluna].var(ddof=1):.4f}\")\n",
    "    print(f\"    Desvio padr√£o: {df_economatica_eda[coluna].std(ddof=0):.4f}\")\n",
    "    print(f\"    Assimetria:  {df_economatica_eda[coluna].skew():.4f}\")\n",
    "    print(f\"    Curtose:  {df_economatica_eda[coluna].kurtosis():.4f}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sobre cada vari√°vel:\n",
    "- **M√≠nimo:** Representa o menor valor dessa coluna no dataframe.\n",
    "- **M√°ximo:** Representa o maior valor dessa coluna no dataframe.\n",
    "- **M√©dia:** Representa a m√©dia aritm√©tica de todos os valores da coluna.\n",
    "- **Amplitude total:** Representa a diferen√ßa entre o valor m√°ximo e m√≠nimo da coluna (max - min), indicando a dispers√£o dos dados.\n",
    "- **Vari√¢ncia:** Mede o grau de dispers√£o dos dados em rela√ß√£o √† m√©dia. Valores altos indicam maior variabilidade.\n",
    "- **Desvio Padr√£o:** Representa a raiz quadrada da vari√¢ncia, medindo a dispers√£o dos dados em rela√ß√£o √† m√©dia na mesma unidade dos dados originais. Quanto maior o desvio padr√£o, mais dispersos est√£o os dados.\n",
    "- **Assimetria (Skewness):** Mede a assimetria da distribui√ß√£o dos dados:\n",
    "  - Valor = 0: distribui√ß√£o sim√©trica\n",
    "  - Valor > 0: assimetria positiva (cauda √† direita)\n",
    "  - Valor < 0: assimetria negativa (cauda √† esquerda)\n",
    "- **Curtose (Kurtosis):** Mede o \"achatamento\" da distribui√ß√£o:\n",
    "  - Valor = 0: distribui√ß√£o normal (mesoc√∫rtica)\n",
    "  - Valor > 0: distribui√ß√£o mais pontiaguda (leptoc√∫rtica)\n",
    "  - Valor < 0: distribui√ß√£o mais achatada (platic√∫rtica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribui√ß√£o dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atrav√© sda visualiza√ß√£o dos resultados acima, podemos dizer que todas as colunas possui uma assimetria enorme, contendo uma esparsidade dos dados enorme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def criar_scatterplot(coluna_y):\n",
    "    \"\"\"\n",
    "    Cria um gr√°fico de dispers√£o usando Plotly\n",
    "    \"\"\"\n",
    "    if not coluna_y:\n",
    "        fig = go.Figure()\n",
    "        fig.update_layout(title=\"Selecione uma coluna para visualizar\")\n",
    "        return fig\n",
    "    \n",
    "    try:\n",
    "        # Remove valores nulos da coluna selecionada\n",
    "        dados_limpos = df_economatica_eda[coluna_y].dropna()\n",
    "        \n",
    "        if dados_limpos.empty:\n",
    "            fig = go.Figure()\n",
    "            fig.update_layout(title=\"Sem dados dispon√≠veis para esta coluna\")\n",
    "            return fig\n",
    "        \n",
    "        # Cria √≠ndices correspondentes aos dados v√°lidos\n",
    "        indices = dados_limpos.index\n",
    "        \n",
    "        # Cria figura Plotly\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Adiciona scatter plot\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=indices,\n",
    "            y=dados_limpos.values,\n",
    "            mode='markers',\n",
    "            name=coluna_y,\n",
    "            marker=dict(\n",
    "                size=6,\n",
    "                color=GRADIO_PRIMARY,\n",
    "                line=dict(\n",
    "                    width=1,\n",
    "                    color=GRADIO_BORDER\n",
    "                )\n",
    "            ),\n",
    "            hovertemplate=f'<b>√çndice</b>: %{{x}}<br><b>{coluna_y}</b>: %{{y:.4f}}<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        # Layout\n",
    "        fig.update_layout(\n",
    "            title=f'Distribui√ß√£o de {coluna_y} por √çndice',\n",
    "            xaxis_title='√çndice',\n",
    "            yaxis_title=coluna_y,\n",
    "            height=500,\n",
    "            showlegend=False,\n",
    "            hovermode='closest',\n",
    "            xaxis=dict(\n",
    "                showgrid=True,\n",
    "                gridwidth=1,\n",
    "                gridcolor=GRADIO_BORDER\n",
    "            ),\n",
    "            yaxis=dict(\n",
    "                showgrid=True,\n",
    "                gridwidth=1,\n",
    "                gridcolor=GRADIO_BORDER\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "        \n",
    "    except Exception as e:\n",
    "        fig = go.Figure()\n",
    "        fig.update_layout(title=f\"Erro ao gerar gr√°fico: {str(e)}\")\n",
    "        return fig\n",
    "\n",
    "# Interface Gradio com Plotly\n",
    "with gr.Blocks(title=\"An√°lise de Dispers√£o\") as interface_scatterplot:\n",
    "    \n",
    "    # Controle superior\n",
    "    dropdown_y = gr.Dropdown(\n",
    "        choices=colunas_numericas, \n",
    "        label=\"Selecione a coluna para visualizar (Eixo Y)\",\n",
    "        value=colunas_numericas[0] if colunas_numericas else None,\n",
    "    )\n",
    "    \n",
    "    # Gr√°fico Plotly\n",
    "    scatter_plot = gr.Plot(\n",
    "        value=criar_scatterplot(colunas_numericas[0] if colunas_numericas else None),\n",
    "        show_label=False\n",
    "    )\n",
    "    \n",
    "    # Event handlers\n",
    "    dropdown_y.change(\n",
    "        fn=criar_scatterplot,\n",
    "        inputs=dropdown_y,\n",
    "        outputs=scatter_plot\n",
    "    )\n",
    "\n",
    "# Lan√ßa a interface\n",
    "interface_scatterplot.launch(share=False, debug=False, inbrowser=False, quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com o Dataframe reordenado, iremos selecionar um range para cada coluna, para que possamos obter os registros tra√ßados como outliers. Nessa etapa iremos selecionar somente os causadores da esparcidade extrema, como mostrado nos histogramas abaixo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_histograma(coluna_selecionada, num_bins=20):\n",
    "    \"\"\"\n",
    "    Cria um gr√°fico Plotly combinando histograma e curva de distribui√ß√£o normal\n",
    "    \"\"\"\n",
    "    if not coluna_selecionada:\n",
    "        fig = go.Figure()\n",
    "        fig.update_layout(title=\"Selecione uma coluna para visualizar\")\n",
    "        return fig\n",
    "    \n",
    "    try:\n",
    "        # Remove valores nulos\n",
    "        dados = df_economatica_eda[coluna_selecionada].dropna()\n",
    "        \n",
    "        if dados.empty:\n",
    "            fig = go.Figure()\n",
    "            fig.update_layout(title=\"Sem dados dispon√≠veis para esta coluna\")\n",
    "            return fig\n",
    "        \n",
    "        # Cria figura Plotly\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Calcula os bins do histograma primeiro para obter a escala correta\n",
    "        counts, bin_edges = np.histogram(dados, bins=num_bins)\n",
    "        bin_width = bin_edges[1] - bin_edges[0]\n",
    "        \n",
    "        # Adiciona histograma\n",
    "        fig.add_trace(go.Histogram(\n",
    "            x=dados,\n",
    "            nbinsx=num_bins,\n",
    "            name='Frequ√™ncia',\n",
    "            marker_color=GRADIO_PRIMARY,\n",
    "            marker_line_color=GRADIO_BORDER,\n",
    "            marker_line_width=1\n",
    "        ))\n",
    "        \n",
    "        # Calcula estat√≠sticas da distribui√ß√£o\n",
    "        mu = dados.mean()\n",
    "        sigma = dados.std()\n",
    "        \n",
    "        # Cria pontos para a curva normal\n",
    "        x_min, x_max = dados.min(), dados.max()\n",
    "        x_curva = np.linspace(x_min, x_max, 200)\n",
    "        y_curva_norm = stats.norm.pdf(x_curva, mu, sigma)\n",
    "        \n",
    "        # Escala a curva normal para corresponder ao histograma\n",
    "        # Multiplica pela √°rea total do histograma (n√∫mero de observa√ß√µes * largura do bin)\n",
    "        y_curva_scaled = y_curva_norm * len(dados) * bin_width\n",
    "        \n",
    "        # Adiciona curva normal escalada\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=x_curva,\n",
    "            y=y_curva_scaled,\n",
    "            mode='lines',\n",
    "            name='Curva Normal',\n",
    "            line=dict(color=GRADIO_SECONDARY, width=3)\n",
    "        ))\n",
    "        \n",
    "        # Layout usando tema configurado globalmente\n",
    "        fig.update_layout(\n",
    "            title=f'Distribui√ß√£o de {coluna_selecionada}',\n",
    "            xaxis_title='Valores',\n",
    "            yaxis_title='Frequ√™ncia',\n",
    "            height=500,\n",
    "            showlegend=True,\n",
    "            legend=dict(\n",
    "                orientation=\"h\",\n",
    "                yanchor=\"bottom\",\n",
    "                y=1.02,\n",
    "                xanchor=\"center\",\n",
    "                x=0.5\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Se der erro, retorna figura vazia com mensagem\n",
    "        fig = go.Figure()\n",
    "        fig.update_layout(title=f\"Erro ao gerar gr√°fico: {str(e)}\")\n",
    "        return fig\n",
    "\n",
    "def resetar_bins():\n",
    "    \"\"\"Retorna valor padr√£o para bins\"\"\"\n",
    "    return 20\n",
    "\n",
    "# Interface Gradio Simplificada\n",
    "with gr.Blocks(title=\"An√°lise de Distribui√ß√£o\") as interface_histograma:\n",
    "    \n",
    "    # Controles superiores\n",
    "    with gr.Row():\n",
    "        dropdown_coluna = gr.Dropdown(\n",
    "            choices=colunas_numericas,\n",
    "            label=\"Selecione a Vari√°vel\",\n",
    "            value=colunas_numericas[0] if colunas_numericas else None,\n",
    "            scale=2,\n",
    "        )\n",
    "        \n",
    "        slider_bins = gr.Slider(\n",
    "            minimum=3,\n",
    "            maximum=50,\n",
    "            value=20,\n",
    "            step=1,\n",
    "            label=\"N√∫mero de Bins\",\n",
    "            scale=1,\n",
    "        )\n",
    "    \n",
    "    # Gr√°fico\n",
    "    histograma = gr.Plot(\n",
    "        value=criar_histograma(colunas_numericas[0] if colunas_numericas else None, 20),\n",
    "        show_label=False\n",
    "    )\n",
    "    \n",
    "    # Event handlers\n",
    "    dropdown_coluna.change(\n",
    "        fn=lambda col, bins: criar_histograma(col, bins),\n",
    "        inputs=[dropdown_coluna, slider_bins],\n",
    "        outputs=histograma\n",
    "    )\n",
    "    \n",
    "    slider_bins.change(\n",
    "        fn=lambda col, bins: criar_histograma(col, bins),\n",
    "        inputs=[dropdown_coluna, slider_bins],\n",
    "        outputs=histograma\n",
    "    )\n",
    "    \n",
    "# Lan√ßa a interface\n",
    "interface_histograma.launch(share=False, debug=False, inbrowser=False, quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criar_boxplot(coluna_y, mostrar_outliers=True, escala_log=False):\n",
    "    \"\"\"\n",
    "    Cria um gr√°fico boxplot usando Plotly\n",
    "    \n",
    "    Args:\n",
    "        coluna_y (str): Nome da coluna para visualizar no eixo Y\n",
    "        mostrar_outliers (bool): Se True, mostra os outliers no boxplot\n",
    "        escala_log (bool): Se True, usa escala logar√≠tmica no eixo Y\n",
    "        \n",
    "    Returns:\n",
    "        go.Figure: Figura Plotly com o boxplot\n",
    "    \"\"\"\n",
    "    if not coluna_y:\n",
    "        fig = go.Figure()\n",
    "        fig.update_layout(title=\"Selecione uma coluna para visualizar\")\n",
    "        return fig\n",
    "    \n",
    "    try:\n",
    "        # Remove valores nulos da coluna selecionada\n",
    "        dados_limpos = df_economatica_eda[coluna_y].dropna()\n",
    "        \n",
    "        if dados_limpos.empty:\n",
    "            fig = go.Figure()\n",
    "            fig.update_layout(title=\"Sem dados dispon√≠veis para esta coluna\")\n",
    "            return fig\n",
    "        \n",
    "        # Verificar se os dados s√£o compat√≠veis com escala logar√≠tmica\n",
    "        tem_valores_negativos = (dados_limpos <= 0).any()\n",
    "        escala_incompativel = False\n",
    "        \n",
    "        if escala_log and tem_valores_negativos:\n",
    "            escala_incompativel = True\n",
    "            escala_log = False\n",
    "        \n",
    "        # Agrupar por ano para comparar distribui√ß√£o por ano\n",
    "        df_agrupado = df_economatica_eda.dropna(subset=[coluna_y])\n",
    "        anos = sorted(df_agrupado['ano'].unique())\n",
    "        \n",
    "        # Cria figura Plotly\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        # Adiciona um boxplot para cada ano\n",
    "        for ano in anos:\n",
    "            dados_ano = df_agrupado[df_agrupado['ano'] == ano][coluna_y]\n",
    "            \n",
    "            if not dados_ano.empty:\n",
    "                fig.add_trace(go.Box(\n",
    "                    y=dados_ano,\n",
    "                    name=f'Ano {ano}',\n",
    "                    boxpoints='outliers' if mostrar_outliers else False,\n",
    "                    jitter=0.3,\n",
    "                    pointpos=-1.8,\n",
    "                    marker=dict(\n",
    "                        color=GRADIO_PRIMARY,\n",
    "                        line=dict(width=1, color=GRADIO_BORDER)\n",
    "                    ),\n",
    "                    line=dict(color=GRADIO_SECONDARY),\n",
    "                    fillcolor='rgba(255, 255, 255, 0.1)',\n",
    "                    hoverinfo='y+name',\n",
    "                    hovertemplate=f'<b>Ano</b>: %{{name}}<br><b>{coluna_y}</b>: %{{y:.4f}}<extra></extra>'\n",
    "                ))\n",
    "        \n",
    "        # Layout usando tema configurado globalmente\n",
    "        titulo = f'Boxplot de {coluna_y} por Ano'\n",
    "        if escala_log:\n",
    "            titulo += \" (Escala Log)\"\n",
    "        elif escala_incompativel:\n",
    "            titulo += \" ‚ö†Ô∏è Escala Log n√£o aplic√°vel: dados cont√™m valores ‚â§ 0\"\n",
    "            \n",
    "        fig.update_layout(\n",
    "            title=titulo,\n",
    "            yaxis_title=coluna_y,\n",
    "            height=600,\n",
    "            showlegend=False,\n",
    "            boxmode='group',\n",
    "            boxgap=0.1,\n",
    "            boxgroupgap=0.2,\n",
    "            yaxis=dict(\n",
    "                showgrid=True,\n",
    "                gridwidth=1,\n",
    "                gridcolor=GRADIO_BORDER,\n",
    "                zeroline=True,\n",
    "                zerolinecolor=GRADIO_BORDER,\n",
    "                zerolinewidth=1,\n",
    "                type='log' if escala_log else 'linear'\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "        \n",
    "    except Exception as e:\n",
    "        fig = go.Figure()\n",
    "        fig.update_layout(title=f\"Erro ao gerar gr√°fico: {str(e)}\")\n",
    "        return fig\n",
    "\n",
    "# Interface Gradio com Plotly - com layout melhorado\n",
    "with gr.Blocks(title=\"An√°lise de Boxplot\") as interface_boxplot:\n",
    "    \n",
    "    # Todos os controles na mesma linha, alinhados verticalmente\n",
    "    with gr.Row(equal_height=True, variant=\"panel\"):\n",
    "        with gr.Column(scale=2):\n",
    "            dropdown_y = gr.Dropdown(\n",
    "                choices=colunas_numericas, \n",
    "                label=\"Selecione a coluna para visualizar\",\n",
    "                value=colunas_numericas[0] if colunas_numericas else None,\n",
    "            )\n",
    "            \n",
    "        with gr.Column(scale=1):\n",
    "            mostrar_outliers = gr.Checkbox(\n",
    "                label=\"Mostrar Outliers\",\n",
    "                value=True,\n",
    "            )\n",
    "            \n",
    "            escala_log = gr.Checkbox(\n",
    "                label=\"Escala Logar√≠tmica\",\n",
    "                value=False,\n",
    "            )\n",
    "    \n",
    "    # Gr√°fico Plotly\n",
    "    boxplot_plot = gr.Plot(\n",
    "        value=criar_boxplot(\n",
    "            colunas_numericas[0] if colunas_numericas else None,\n",
    "            True,\n",
    "            False\n",
    "        ),\n",
    "        show_label=False\n",
    "    )\n",
    "    \n",
    "    # Event handlers\n",
    "    dropdown_y.change(\n",
    "        fn=criar_boxplot,\n",
    "        inputs=[dropdown_y, mostrar_outliers, escala_log],\n",
    "        outputs=boxplot_plot\n",
    "    )\n",
    "    \n",
    "    mostrar_outliers.change(\n",
    "        fn=criar_boxplot,\n",
    "        inputs=[dropdown_y, mostrar_outliers, escala_log],\n",
    "        outputs=boxplot_plot\n",
    "    )\n",
    "    \n",
    "    escala_log.change(\n",
    "        fn=criar_boxplot,\n",
    "        inputs=[dropdown_y, mostrar_outliers, escala_log],\n",
    "        outputs=boxplot_plot\n",
    "    )\n",
    "\n",
    "# Lan√ßa a interface\n",
    "interface_boxplot.launch(share=False, debug=False, inbrowser=False, quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identificar_outliers_consolidados(df):\n",
    "    \"\"\"\n",
    "    Identifica outliers em todas as colunas num√©ricas usando o m√©todo IQR\n",
    "    e retorna um DataFrame consolidado com todos os outliers.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame com os dados\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame consolidado com todos os outliers\n",
    "    \"\"\"\n",
    "    # Lista para armazenar todos os outliers\n",
    "    outliers_list = []\n",
    "    \n",
    "    # Identifica colunas num√©ricas (excluindo identificadores e categ√≥ricas)\n",
    "    colunas_numericas = [col for col in df.columns \n",
    "                        if col not in ['nome', 'codigo', 'setor', 'pandemia', 'ano'] \n",
    "                        and df[col].dtype in ['int64', 'float64', 'int32', 'float32']]\n",
    "    \n",
    "    print(f\"Analisando outliers em {len(colunas_numericas)} colunas num√©ricas...\")\n",
    "    \n",
    "    # Para cada coluna num√©rica, identifica outliers\n",
    "    for coluna in colunas_numericas:\n",
    "        # Calcula quartis e IQR\n",
    "        Q1 = df[coluna].quantile(0.25)\n",
    "        Q3 = df[coluna].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Define limites para outliers\n",
    "        limite_inferior = Q1 - 1.5 * IQR\n",
    "        limite_superior = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Identifica registros que s√£o outliers\n",
    "        outliers_mask = (df[coluna] < limite_inferior) | (df[coluna] > limite_superior)\n",
    "        outliers_df = df[outliers_mask].copy()\n",
    "        \n",
    "        if not outliers_df.empty:\n",
    "            # Para cada outlier, adiciona √† lista\n",
    "            for idx, row in outliers_df.iterrows():\n",
    "                outliers_list.append({\n",
    "                    'codigo': row['codigo'] if 'codigo' in df.columns else None,\n",
    "                    'nome': row['nome'] if 'nome' in df.columns else None,\n",
    "                    'setor': row['setor'] if 'setor' in df.columns else None,\n",
    "                    'ano': row['ano'] if 'ano' in df.columns else None,\n",
    "                    'variavel': coluna,\n",
    "                    'valor': row[coluna],\n",
    "                    'limite_inferior': limite_inferior,\n",
    "                    'limite_superior': limite_superior,\n",
    "                    'is_baixo': row[coluna] < limite_inferior,\n",
    "                    'is_alto': row[coluna] > limite_superior\n",
    "                })\n",
    "    \n",
    "    # Cria DataFrame com todos os outliers\n",
    "    outliers_consolidados = pd.DataFrame(outliers_list)\n",
    "    \n",
    "    print(f\"Total de outliers identificados: {len(outliers_consolidados)}\")\n",
    "    print(f\"Em {outliers_consolidados['variavel'].nunique()} vari√°veis diferentes\")\n",
    "    if 'codigo' in df.columns:\n",
    "        print(f\"Afetando {outliers_consolidados['codigo'].nunique()} c√≥digos √∫nicos\")\n",
    "    \n",
    "    return outliers_consolidados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o DataFrame de outliers\n",
    "df_economatica_outliers = identificar_outliers_consolidados(df_economatica_eda)\n",
    "\n",
    "# Mostra resumo dos outliers por vari√°vel\n",
    "print(\"\\n=== RESUMO DOS OUTLIERS POR VARI√ÅVEL ===\")\n",
    "resumo_outliers = df_economatica_outliers.groupby('variavel').size().sort_values(ascending=False)\n",
    "print(resumo_outliers)\n",
    "\n",
    "# Verifica quais empresas t√™m mais outliers\n",
    "if 'codigo' in df_economatica_eda.columns:\n",
    "    print(\"\\n=== EMPRESAS COM MAIS OUTLIERS ===\")\n",
    "    top_empresas = df_economatica_outliers['codigo'].value_counts().head(10)\n",
    "    print(top_empresas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "√â, dado a quantidade de registros que temos, o ideal √© n√£o tratar mesmo..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HYuJdprXX9CU"
   },
   "source": [
    "# Pr√©-processamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normaliza√ß√£o dos nomes das colunas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criado fun√ß√µes para resolver problemas de duplica√ß√£o de colunas e padroniza√ß√£o nos nomes das colunas, ser√£o utilizadas em breve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slugify(txt: str) -> str:\n",
    "    \"\"\"\n",
    "    Converte uma string para um formato \"slug\", amig√°vel para nomes de coluna.\n",
    "    Remove acentos, substitui caracteres n√£o alfanum√©ricos por underscores\n",
    "    e limpa underscores extras.\n",
    "\n",
    "    Args:\n",
    "        txt (str): A string de entrada.\n",
    "\n",
    "    Returns:\n",
    "        str: A string convertida para formato slug, ou \"missing\" se a entrada for nula ou vazia ap√≥s a limpeza.\n",
    "    \"\"\"\n",
    "    if txt is None:\n",
    "        return \"missing\"\n",
    "    txt = f'setor_{str(txt).lower().strip()}'\n",
    "    # remover acentos\n",
    "    txt = unicodedata.normalize(\"NFKD\", txt).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    # trocar n√£o-alfanum√©ricos por \"_\"\n",
    "    txt = re.sub(r\"[^a-z0-9]+\", \"_\", txt)\n",
    "    # limpar underscores duplicados e bordas\n",
    "    txt = re.sub(r\"_+\", \"_\", txt).strip(\"_\")\n",
    "    return txt or \"missing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTzB5Cq7eeUV"
   },
   "source": [
    "## Gera CSV com os setores para agrupamento manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica slugify na coluna setor ANTES de filtrar e gerar o CSV\n",
    "df_economatica_sem_nulos_slugified = df_economatica_sem_nulos.copy()\n",
    "df_economatica_sem_nulos_slugified['setor'] = df_economatica_sem_nulos_slugified['setor'].apply(slugify)\n",
    "\n",
    "# Filtra dados de 2020 e extrai setores √∫nicos (agora j√° com slugify aplicado)\n",
    "df_setores_2020 = df_economatica_sem_nulos_slugified[df_economatica_sem_nulos_slugified['ano'] == 2020][['setor']].drop_duplicates()\n",
    "\n",
    "# Define o caminho baseado no ambiente\n",
    "if is_colab:\n",
    "    caminho_arquivo = '/content/setores_economatica_2020.csv'\n",
    "else:\n",
    "    os.makedirs('../datasets', exist_ok=True)\n",
    "    caminho_arquivo = '../datasets/setores_economatica_2020.csv'\n",
    "\n",
    "# Exporta o CSV\n",
    "df_setores_2020.to_csv(caminho_arquivo, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"‚úÖ CSV exportado: {caminho_arquivo}\")\n",
    "print(f\"üìä {len(df_setores_2020)} empresas | {df_setores_2020['setor'].nunique()} setores √∫nicos\")\n",
    "print(f\"\\nPrimeiras linhas:\")\n",
    "print(df_setores_2020.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBjkxhAwffQ9"
   },
   "source": [
    "## Upa CSV com os setores agrupados e substitui os setores atrav√©s do mapeamento dos grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL do CSV com o mapeamento dos setores\n",
    "url_mapeamento = 'https://drive.google.com/file/d/1ITrtp7GsfZi9eZVTDQgro8fMu23zZu0w/view' \n",
    "\n",
    "# Define o caminho baseado no ambiente\n",
    "if is_colab:\n",
    "    output_mapeamento = '/content/mapeamento_setores.csv'\n",
    "else:\n",
    "    output_mapeamento = '../datasets/mapeamento_setores.csv'\n",
    "\n",
    "# Baixa o arquivo de mapeamento\n",
    "print(f\"Baixando mapeamento de setores para: {output_mapeamento}\")\n",
    "gdown.download(url_mapeamento, output_mapeamento, fuzzy=True)\n",
    "\n",
    "# Carrega o CSV de mapeamento\n",
    "df_mapeamento = pd.read_csv(output_mapeamento, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CSV de mapeamento carregado:\")\n",
    "print(df_mapeamento.head())\n",
    "print(f\"\\nShape: {df_mapeamento.shape}\")\n",
    "print(f\"Colunas: {list(df_mapeamento.columns)}\")\n",
    "\n",
    "# Cria o dicion√°rio de mapeamento setor -> grupo\n",
    "mapeamento_setores = dict(zip(df_mapeamento['setor'], df_mapeamento['grupo']))\n",
    "\n",
    "print(f\"\\nMapeamento criado com {len(mapeamento_setores)} setores:\")\n",
    "for setor, grupo in list(mapeamento_setores.items())[:5]:  # Mostra apenas os primeiros 5\n",
    "    print(f\"  {setor} -> {grupo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica slugify na coluna setor do DataFrame principal antes do mapeamento\n",
    "df_pre_processado = df_economatica_sem_nulos.copy()\n",
    "\n",
    "df_pre_processado['setor'] = df_pre_processado['setor'].apply(slugify)\n",
    "\n",
    "df_pre_processado.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplica o mapeamento no DataFrame principal\n",
    "df_pre_processado['setor'] = df_pre_processado['setor'].map(mapeamento_setores)\n",
    "\n",
    "df_pre_processado.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYYhYFIffS96"
   },
   "outputs": [],
   "source": [
    "# Verifica se h√° setores n√£o mapeados (NaN)\n",
    "setores_nao_mapeados = df_pre_processado['setor'].isnull().sum()\n",
    "if setores_nao_mapeados > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è Aten√ß√£o: {setores_nao_mapeados} registros com setores n√£o mapeados\")\n",
    "    setores_perdidos = df_pre_processado[df_pre_processado['setor'].isnull()]['codigo'].unique()\n",
    "    print(f\"C√≥digos afetados: {setores_perdidos[:5]}...\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Todos os setores foram mapeados com sucesso!\")\n",
    "\n",
    "# Mostra a distribui√ß√£o dos novos grupos\n",
    "print(f\"\\nDistribui√ß√£o dos grupos ap√≥s mapeamento:\")\n",
    "print(df_pre_processado['setor'].value_counts())\n",
    "\n",
    "print(f\"\\nTotal de grupos √∫nicos: {df_pre_processado['setor'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo o label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicia o label encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Executa o label encoder na coluna setor\n",
    "df_pre_processado['setor'] = le.fit_transform(df_pre_processado['setor'])\n",
    "df_pre_processado['codigo'] = le.fit_transform(df_pre_processado['codigo'])\n",
    "\n",
    "print(\"Category Mapping:\", le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_processado.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removendo a coluna nome, codigo e ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_processado.drop(columns=['nome', 'codigo', 'ano'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def aplicar_minmax_normalizacao(df):\n",
    "    \"\"\"\n",
    "    Aplica MinMaxScaler (0-1) para todas as features num√©ricas.\n",
    "    Preserva colunas categ√≥ricas e identificadoras.\n",
    "    \"\"\"\n",
    "    print(\"=== NORMALIZA√á√ÉO MINMAX (0-1) ===\\n\")\n",
    "    \n",
    "    df_normalizado = df.copy()\n",
    "    \n",
    "    # Identifica colunas num√©ricas para normalizar\n",
    "    colunas_numericas = [col for col in df.columns \n",
    "                        if col not in ['setor', 'ano', 'pandemia', 'variacao_valor_mercado'] \n",
    "                        and df[col].dtype in ['int64', 'float64', 'int32', 'float32']]\n",
    "    \n",
    "    print(f\"üîß Normalizando {len(colunas_numericas)} features num√©ricas...\")\n",
    "    \n",
    "    # Aplica MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Transforma apenas as colunas num√©ricas\n",
    "    df_normalizado[colunas_numericas] = scaler.fit_transform(df_normalizado[colunas_numericas])\n",
    "    \n",
    "    print(f\"‚úÖ Normaliza√ß√£o conclu√≠da!\")\n",
    "    print(f\"   - Todas as features est√£o na escala [0, 1]\")\n",
    "    print(f\"   - Colunas preservadas: setor, ano, pandemia, variacao_valor_mercado\")\n",
    "    \n",
    "    # Estat√≠sticas p√≥s-normaliza√ß√£o\n",
    "    print(f\"\\nüìä VERIFICA√á√ÉO P√ìS-NORMALIZA√á√ÉO:\")\n",
    "    stats_pos = df_normalizado[colunas_numericas].describe()\n",
    "    print(f\"   - Valor m√≠nimo global: {stats_pos.loc['min'].min():.6f}\")\n",
    "    print(f\"   - Valor m√°ximo global: {stats_pos.loc['max'].max():.6f}\")\n",
    "    print(f\"   - M√©dia das m√©dias: {stats_pos.loc['mean'].mean():.6f}\")\n",
    "    \n",
    "    # Mostra amostra das primeiras features normalizadas\n",
    "    print(f\"\\nüëÄ AMOSTRA DAS PRIMEIRAS 5 FEATURES NORMALIZADAS:\")\n",
    "    for col in colunas_numericas[:5]:\n",
    "        valores = df_normalizado[col]\n",
    "        print(f\"   - {col}: min={valores.min():.3f}, max={valores.max():.3f}, mean={valores.mean():.3f}\")\n",
    "    \n",
    "    return df_normalizado, scaler, colunas_numericas\n",
    "\n",
    "def comparar_antes_depois_minmax(df_original, df_normalizado, colunas_numericas):\n",
    "    \"\"\"\n",
    "    Compara estat√≠sticas antes e depois da normaliza√ß√£o MinMax.\n",
    "    \"\"\"\n",
    "    print(\"\\n=== COMPARA√á√ÉO ANTES vs DEPOIS ===\\n\")\n",
    "    \n",
    "    print(f\"{'Feature':<20} {'Antes_Min':<12} {'Antes_Max':<12} {'Depois_Min':<12} {'Depois_Max':<12}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for col in colunas_numericas[:10]:  # Mostra apenas as primeiras 10\n",
    "        antes_min = df_original[col].min()\n",
    "        antes_max = df_original[col].max()\n",
    "        depois_min = df_normalizado[col].min()\n",
    "        depois_max = df_normalizado[col].max()\n",
    "        \n",
    "        print(f\"{col:<20} {antes_min:<12.2f} {antes_max:<12.2f} {depois_min:<12.6f} {depois_max:<12.6f}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Todas as features foram normalizadas para a escala [0, 1]\")\n",
    "\n",
    "# Aplicar normaliza√ß√£o MinMax\n",
    "print(\"=== APLICANDO NORMALIZA√á√ÉO MINMAX ===\\n\")\n",
    "\n",
    "# Aplicar MinMaxScaler\n",
    "df_pre_processado_normalizado, scaler_usado, features_normalizadas = aplicar_minmax_normalizacao(df_pre_processado)\n",
    "\n",
    "# Comparar antes e depois\n",
    "comparar_antes_depois_minmax(df_pre_processado, df_pre_processado_normalizado, features_normalizadas)\n",
    "\n",
    "# Verifica√ß√£o final\n",
    "print(f\"\\nüìã RESUMO FINAL:\")\n",
    "print(f\"   - Shape do dataset: {df_pre_processado_normalizado.shape}\")\n",
    "print(f\"   - Features normalizadas: {len(features_normalizadas)}\")\n",
    "print(f\"   - M√©todo usado: MinMaxScaler [0, 1]\")\n",
    "print(f\"   - Dados prontos para modelagem: ‚úÖ\")\n",
    "\n",
    "# Mostra amostra final\n",
    "print(f\"\\nüëÄ AMOSTRA DO DATASET NORMALIZADO:\")\n",
    "print(df_pre_processado_normalizado.head())\n",
    "\n",
    "df_pre_processado_normalizado['variacao_valor_mercado'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gerar o dataset pr√©-processado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pre_processado_normalizado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar CSV do dataset pr√©-processado e normalizado\n",
    "print(\"=== EXPORTA√á√ÉO DO DATASET PR√â-PROCESSADO E NORMALIZADO ===\\n\")\n",
    "\n",
    "# Define o caminho baseado no ambiente\n",
    "if is_colab:\n",
    "    caminho_csv_normalizado = '/content/df_economatica_pre_processado_normalizado.csv'\n",
    "else:\n",
    "    os.makedirs('../datasets', exist_ok=True)\n",
    "    caminho_csv_normalizado = '../datasets/df_economatica_pre_processado_normalizado.csv'\n",
    "\n",
    "# Exporta o CSV\n",
    "df_pre_processado_normalizado.to_csv(caminho_csv_normalizado, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"‚úÖ Dataset pr√©-processado e normalizado exportado para: {caminho_csv_normalizado}\")\n",
    "print(f\"üìä Shape do dataset: {df_pre_processado_normalizado.shape}\")\n",
    "print(f\"üìÖ Per√≠odo: {df_pre_processado_normalizado['ano'].min()} - {df_pre_processado_normalizado['ano'].max()}\")\n",
    "print(f\"üè¢ Empresas √∫nicas: {df_pre_processado_normalizado['setor'].nunique()} setores\")\n",
    "print(f\"üìà Colunas: {df_pre_processado_normalizado.shape[1]}\")\n",
    "\n",
    "# Resumo das colunas por tipo\n",
    "print(f\"\\nüìã RESUMO DAS COLUNAS:\")\n",
    "colunas_por_tipo = {\n",
    "    'Categ√≥ricas': ['setor'],\n",
    "    'Temporais': ['ano', 'pandemia'],\n",
    "    'Target': ['variacao_valor_mercado'],\n",
    "    'Features Normalizadas': [col for col in df_pre_processado_normalizado.columns \n",
    "                             if col not in ['setor', 'ano', 'pandemia', 'variacao_valor_mercado']]\n",
    "}\n",
    "\n",
    "for tipo, colunas in colunas_por_tipo.items():\n",
    "    colunas_existentes = [col for col in colunas if col in df_pre_processado_normalizado.columns]\n",
    "    print(f\"  - {tipo}: {len(colunas_existentes)} colunas\")\n",
    "    if tipo == 'Features Normalizadas' and len(colunas_existentes) > 0:\n",
    "        print(f\"    Exemplos: {colunas_existentes[:5]}\")\n",
    "\n",
    "print(f\"\\nüîç VERIFICA√á√ïES FINAIS:\")\n",
    "print(f\"  - Valores nulos: {df_pre_processado_normalizado.isnull().sum().sum()}\")\n",
    "print(f\"  - Linhas duplicadas: {df_pre_processado_normalizado.duplicated().sum()}\")\n",
    "print(f\"  - Registros por empresa (anos): {df_pre_processado_normalizado.groupby('setor').size().describe()}\")\n",
    "\n",
    "# Estat√≠sticas das features normalizadas\n",
    "features_normalizadas = [col for col in df_pre_processado_normalizado.columns \n",
    "                        if col not in ['setor', 'ano', 'pandemia', 'variacao_valor_mercado']]\n",
    "\n",
    "if features_normalizadas:\n",
    "    print(f\"\\nüìä ESTAT√çSTICAS DAS FEATURES NORMALIZADAS:\")\n",
    "    stats_normalizadas = df_pre_processado_normalizado[features_normalizadas].describe()\n",
    "    print(f\"  - Valor m√≠nimo global: {stats_normalizadas.loc['min'].min():.6f}\")\n",
    "    print(f\"  - Valor m√°ximo global: {stats_normalizadas.loc['max'].max():.6f}\")\n",
    "    print(f\"  - M√©dia global: {stats_normalizadas.loc['mean'].mean():.6f}\")\n",
    "    print(f\"  - Desvio padr√£o m√©dio: {stats_normalizadas.loc['std'].mean():.6f}\")\n",
    "\n",
    "# Distribui√ß√£o da vari√°vel target\n",
    "print(f\"\\nüéØ DISTRIBUI√á√ÉO DA VARI√ÅVEL TARGET:\")\n",
    "target_dist = df_pre_processado_normalizado['variacao_valor_mercado'].value_counts().sort_index()\n",
    "print(f\"  - Classe 0 (desvaloriza√ß√£o): {target_dist.get(0, 0):,} registros\")\n",
    "print(f\"  - Classe 1 (valoriza√ß√£o): {target_dist.get(1, 0):,} registros\")\n",
    "if len(target_dist) > 0:\n",
    "    total_validos = target_dist.sum()\n",
    "    print(f\"  - Balanceamento: {(target_dist.get(1, 0) / total_validos * 100):.1f}% valoriza√ß√£o\")\n",
    "\n",
    "# Distribui√ß√£o por ano\n",
    "print(f\"\\nüìÖ DISTRIBUI√á√ÉO POR ANO:\")\n",
    "distribuicao_ano = df_pre_processado_normalizado['ano'].value_counts().sort_index()\n",
    "for ano, count in distribuicao_ano.items():\n",
    "    print(f\"  - {ano}: {count:,} registros\")\n",
    "\n",
    "# Mostra amostra dos dados finais\n",
    "print(f\"\\nüëÄ AMOSTRA DOS DADOS FINAIS:\")\n",
    "print(df_pre_processado_normalizado.head())\n",
    "\n",
    "print(f\"\\nüìÅ Arquivo salvo com sucesso!\")\n",
    "print(f\"   Caminho: {caminho_csv_normalizado}\")\n",
    "print(f\"   Tamanho estimado: ~{df_pre_processado_normalizado.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "print(f\"   Pronto para modelagem: ‚úÖ\")\n",
    "\n",
    "# Informa√ß√µes adicionais para modelagem\n",
    "print(f\"\\nüöÄ INFORMA√á√ïES PARA MODELAGEM:\")\n",
    "print(f\"  - Todas as features num√©ricas normalizadas: [0, 1]\")\n",
    "print(f\"  - Setor: Label encoded (categ√≥rica)\")\n",
    "print(f\"  - Ano: Num√©rico (2021-2024)\")\n",
    "print(f\"  - Pandemia: Bin√°ria (0/1)\")\n",
    "print(f\"  - Target: Bin√°ria (0/1) - Classifica√ß√£o\")\n",
    "print(f\"  - Sem valores nulos\")\n",
    "print(f\"  - Sem duplicatas\")\n",
    "print(f\"  - Dataset balanceado dispon√≠vel para split treino/teste\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gjrYXg4s3U72",
    "CoA5Cec6Ci3D",
    "Wv3lmdUOEAK8",
    "HYuJdprXX9CU",
    "tWidkFilRP7n",
    "0-kbPSkmWout",
    "DIMkFQiLBkc6"
   ],
   "provenance": [],
   "runtime_attributes": {
    "runtime_version": "2025.07"
   }
  },
  "kernelspec": {
   "display_name": "economatica-eJafRn9J",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
